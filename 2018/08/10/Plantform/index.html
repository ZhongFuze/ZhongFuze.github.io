<!DOCTYPE html><html><head><meta charset="utf-8"><title>Plantform | F泽泽馥泽泽杂记F</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="description" content="Mapreduce,Hive,Spark平台搭建说明平台搭建成功后，使用Scala语言进行算法设计和应用的开发。文末有使用scala设计的Item-Based Collaboration Filtering和SlopeOne 两个协同过滤算法一、搭建准备VMWare WorkstationUbuntu 14.04 Server.isoXshell——远程连接主机终端Server 版安装配置新建三台"><meta name="keywords" content="spark,hive,hadoop,scala"><meta property="og:type" content="article"><meta property="og:title" content="Plantform"><meta property="og:url" content="http://zhongfuze.github.io/2018/08/10/Plantform/index.html"><meta property="og:site_name" content="F泽泽馥泽泽杂记F"><meta property="og:description" content="Mapreduce,Hive,Spark平台搭建说明平台搭建成功后，使用Scala语言进行算法设计和应用的开发。文末有使用scala设计的Item-Based Collaboration Filtering和SlopeOne 两个协同过滤算法一、搭建准备VMWare WorkstationUbuntu 14.04 Server.isoXshell——远程连接主机终端Server 版安装配置新建三台"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-f6a9ce17b3444f5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-ba2cf897a710d10a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-57aefff236e27576.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-86f8934ecb41c6bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-4ef3b55488645496.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-57019829b1015811.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-2d7a6e55039693be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-082bb6fcae008cf6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-b9db3ff25ec36371.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-1c31a45ad9b9bca6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-7b362e81b839dc9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-f9a73d0140a2a540.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-bc2628e8c05111dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-60c0f8ea5ea9b781.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-e8388857afce784f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-6525d52795b26ae3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2560767-a753be3edc411cd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:updated_time" content="2018-08-10T08:09:39.290Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Plantform"><meta name="twitter:description" content="Mapreduce,Hive,Spark平台搭建说明平台搭建成功后，使用Scala语言进行算法设计和应用的开发。文末有使用scala设计的Item-Based Collaboration Filtering和SlopeOne 两个协同过滤算法一、搭建准备VMWare WorkstationUbuntu 14.04 Server.isoXshell——远程连接主机终端Server 版安装配置新建三台"><meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/2560767-f6a9ce17b3444f5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><link rel="icon" href="/favicon.png"><link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700" rel="stylesheet"><link rel="stylesheet" href="/icomoon/style.css"><link rel="stylesheet" href="/style.css"></head><body><div class="site-wrapper"><div id="loading-bar-wrapper"><div id="loading-bar"></div></div><script>document.getElementById("loading-bar").style.width="20%"</script><header id="header" class="site-header clearfix"><a class="logo square clearfix" href="/"><span class="b">F </span><span class="w">泽 </span><span class="b">泽 </span><span class="b">馥 </span><span class="b">泽 </span><span class="b">泽 </span><span class="b">杂 </span><span class="w">记 </span><span class="b">F </span></a><a class="me square site-nav-switch clearfix"><span class="b"><span class="icon icon-menu"></span></span></a></header><script>document.getElementById("loading-bar").style.width="40%"</script><main id="main" class="clearfix"><article id="post-Plantform" class="article white-box article-type-post" itemscope itemprop="blogPost"><header class="article-header"><h1 class="article-title" itemprop="name">Plantform</h1><div class="article-meta">Posted on <time class="article-time" datetime="2018-08-10T07:43:31.000Z" itemprop="datePublished">8月 10, 2018</time></div></header><div class="article-entry" itemprop="articleBody"><h2 id="Mapreduce-Hive-Spark平台搭建"><a href="#Mapreduce-Hive-Spark平台搭建" class="headerlink" title="Mapreduce,Hive,Spark平台搭建"></a>Mapreduce,Hive,Spark平台搭建</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>平台搭建成功后，使用Scala语言进行算法设计和应用的开发。文末有使用scala设计的Item-Based Collaboration Filtering和SlopeOne 两个协同过滤算法</p><h3 id="一、搭建准备"><a href="#一、搭建准备" class="headerlink" title="一、搭建准备"></a><strong>一、搭建准备</strong></h3><p>VMWare Workstation</p><p>Ubuntu 14.04 Server.iso</p><p>Xshell——远程连接主机终端</p><p><strong>Server 版安装配置</strong><br>新建三台虚拟机，安装时选择OpenSHH Server预安装环境</p><p>一台作为master</p><p>另两台作为slave，命名为slave1和slave2</p><p>master的配置比slave要稍微高一些</p><p><strong>IP 设置</strong><br>VMnet8是VM配置的虚拟网卡</p><p>设置VMnet8，选择「NAT 模式」</p><p>设置子网IP （192.168.142.0）和子网掩码（255.255.255.0）</p><p><strong>Ubuntu 系统的登录和 IP 的固化</strong></p><p>输入 <code>ifconfig</code> 回车查询当前的 ip 地址，准备将其长期绑定到当前虚拟机,为了方便后续实验。</p><blockquote><p>master的ip：192.168.142.128</p><p>slave1的ip：192.168.142.130</p><p>slave2的ip：192.168.142.132</p></blockquote><p>此时的 IP 地址是由 DHCP 服务器动态分配的，为了让这个 IP 地址能一直与这台<br>虚拟机绑定，我们需要改变系统启动时获取 IP 的方式，从 DHCP 切换到静态 IP<br>地址，为此需要编辑 Linux 的网卡配置文件（/etc/network/interfaces），输入命令</p><p><code>sudo vi /etc/network/interfaces</code></p><p>回车，可以看到 eth0 的 IP 获取方式是 dhcp：</p><p>此时需要修改 ip 的获取方式从 DHCP 到 static，设置静态的 ip 地址、子网掩码和<br>默认网关。</p><p>把这个文件更改为：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#This file describes the network interfaces available on your system</span></span><br><span class="line"><span class="comment"># and how to activate them. For more information, see interfaces(5).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The loopback network interface</span></span><br><span class="line">auto lo</span><br><span class="line">iface lo inet loopback</span><br><span class="line"></span><br><span class="line"><span class="comment"># The primary network interface</span></span><br><span class="line">auto eth0</span><br><span class="line">iface eth0 inet static</span><br><span class="line">address 192.168.142.128</span><br><span class="line">gateway 192.168.142.2</span><br><span class="line">netmask 255.255.255.0</span><br></pre></td></tr></table></figure><p>为了进一步固化 DNS 服务器 IP，输入</p><p><code>sudo vi /etc/resolvconf/resolv.conf.d/bas</code></p><p>将里面的内容替换成<code>nameserver 192.168.142.2</code>，vi 保存</p><p><strong>系统的登录和 IP 的固化重启后完成</strong></p><p><strong>使用Xshell连接三个虚拟机终端</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-f6a9ce17b3444f5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Xshell.png"></p><p><strong>下载lrzsz工具</strong><br>用于虚拟机与宿主机之间文件的上传和下载</p><p><code>sudo apt-get update</code></p><p><code>sudo apt-get install lrzsz</code></p><p>rz命令，从宿主机传文件至虚拟机</p><p>sz命令，从虚拟机获取文件至宿主机</p><h3 id="二、Hadoop-及相关环境的安装"><a href="#二、Hadoop-及相关环境的安装" class="headerlink" title="二、Hadoop 及相关环境的安装"></a><strong>二、Hadoop 及相关环境的安装</strong></h3><p>开源分布式平台 Hadoop 可以聚合多个计算机形成集群，在各个节点上安装配置完Hadoop后可以直接提交分布式代码到集群计算。本次实验可以在个人电脑上用VMware完成，或使用天河二号上的 OpenStack平台创建内存为2G的虚拟机资源完成。</p><p><strong>安装包准备</strong></p><p>Hadoop环境——hadoop-2.6.0.tar.gz</p><p>Java环境——jdk-8u60-linux-x64.tar.gz</p><p><strong>虚拟机ip配置测试</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/hosts #编辑 /etc/hosts 文件，插入角色与<span class="built_in"> IP </span>映射</span><br><span class="line"></span><br><span class="line">ping master -c 4 #尝试用角色名<span class="built_in"> ping </span>其它主机，一次 4 个包</span><br></pre></td></tr></table></figure><p>hosts文件修改为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">127<span class="selector-class">.0</span><span class="selector-class">.0</span><span class="selector-class">.1</span>       <span class="selector-tag">localhost</span></span><br><span class="line"></span><br><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.142</span><span class="selector-class">.128</span> <span class="selector-tag">master</span></span><br><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.142</span><span class="selector-class">.130</span> <span class="selector-tag">slave1</span></span><br><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.142</span><span class="selector-class">.132</span> <span class="selector-tag">slave2</span></span><br><span class="line"></span><br><span class="line"># <span class="selector-tag">The</span> <span class="selector-tag">following</span> <span class="selector-tag">lines</span> <span class="selector-tag">are</span> <span class="selector-tag">desirable</span> <span class="selector-tag">for</span> <span class="selector-tag">IPv6</span> <span class="selector-tag">capable</span> <span class="selector-tag">hosts</span></span><br><span class="line"><span class="selector-pseudo">::1</span>     <span class="selector-tag">localhost</span> <span class="selector-tag">ip6-localhost</span> <span class="selector-tag">ip6-loopback</span></span><br><span class="line"><span class="selector-tag">ff02</span><span class="selector-pseudo">::1</span> <span class="selector-tag">ip6-allnodes</span></span><br><span class="line"><span class="selector-tag">ff02</span><span class="selector-pseudo">::2</span> <span class="selector-tag">ip6-allrouters</span></span><br></pre></td></tr></table></figure><p>三个虚拟机能够使用主机名（不是ip）ping通即配置正确</p><p><strong>配置 SSH 无密码登录</strong><br>保障了 Hadoop 可以通过角色名在局域网里找到各个节点，为了让 Hadoop 可以进<br>一步读取、操作各个节点，需要赋予其登录的权限，意即让 Hadoop 拥有各个节点的普通用户账<br>号，从而在需要操作各个节点时直接用对应的账号登录获取操作权限。SSH 协议可以为节点上的<br>账户创建唯一的公私钥，然后利用这些公私钥实现无密码登录，从而让 Hadoop 直接绕开传统的<br>账号密码登录过程，直接用公私钥访问节点。</p><p>生成各个节点的 SSH 公私钥：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> ~<span class="string">/.ssh</span> <span class="comment"># 如果没有该目录，先执行一次 ssh localhost</span></span><br><span class="line">rm <span class="string">./id_rsa</span>* <span class="comment"># 删除之前生成的公匙（如果有）</span></span><br><span class="line">ssh-keygen -t rsa <span class="comment"># 一直按回车就可以</span></span><br></pre></td></tr></table></figure><p>为了让每个节点都拥有其它节点的公钥，要先把所有公钥放进一个文件里</p><ol><li><p>在 master 上，将 master 的公钥复制到 authorized_keys 文件里：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ./id_rsa.pub <span class="meta">&gt;&gt; </span>./authorized_keys <span class="comment"># cat 命令用于提取内容，&gt;&gt;输出重定向</span></span><br></pre></td></tr></table></figure></li><li><p>将 slave1、slave2 的公钥文件发送给 master，此时的传送要输入密码：</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp ~<span class="regexp">/.ssh/id</span>_rsa.pub hadoop<span class="variable">@master</span><span class="symbol">:/home/hadoop/</span></span><br></pre></td></tr></table></figure></li><li><p>Master 将接收到的 slave1 的公钥文件里的内容提取追加到 authorized_keys 文<br>件里：</p></li></ol><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~<span class="string">/id_rsa.pub</span> &gt;&gt; ~<span class="string">/.ssh/authorized_keys</span></span><br></pre></td></tr></table></figure><ol><li>将 slave2 的公钥内容也放进 authorized_keys 文件，然后将<br>authorized_keys 文件分别发送到两个 slave 的~/.ssh/下：<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp ~<span class="regexp">/.ssh/authorized</span>_keys hadoop<span class="variable">@slave1</span><span class="symbol">:/home/hadoop/</span>.ssh/</span><br></pre></td></tr></table></figure></li></ol><p>搭建成功表现：每个节点尝试使用 ssh &lt;角色名&gt;的命令直接登录其它节点，直到每个节点都可以<br>成功免密码登录其它节点，则免密码登录配置成功！ 如在 master 上输入：<code>ssh slave1</code>即可直接登陆slave1的虚拟机，不需要再输入密码登陆，便于后续实验操作。</p><p><strong>配置 Java环境和Hadoop环境</strong></p><p>安装包准备：<br>jdk-8u60-linux-x64.tar</p><p>hadoop-2.6.0.tar.gz</p><ol><li><p>安装 JDK<br>将上传的 JDK 压缩包（jdk-8u60-linux-x64.tar）放到根目录<code>/home/hadoop/</code>，解<br>压并放到指定的文件夹：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -<span class="selector-tag">p</span> /usr/local/jvm</span><br><span class="line">tar -zxvf jdk-<span class="number">8</span>u60-linux-x64<span class="selector-class">.tar</span><span class="selector-class">.gz</span> -C /usr/local/jvm</span><br></pre></td></tr></table></figure></li><li><p>安装 Hadoop</p></li></ol><p>在各个节点上将 hadoop 解压到<code>/usr/local/</code>目录下，改变其所属用户和所属组（让<br>hadoop 软件用 hadoop 账号登录时对 hadoop 文件夹拥有最高权限）：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop<span class="number">-2.6</span><span class="number">.0</span>.tar.gz -C <span class="meta-keyword">/usr/</span>local/</span><br><span class="line">sudo mv <span class="meta-keyword">/usr/</span>loca/hadoop<span class="number">-2.6</span><span class="number">.0</span> <span class="meta-keyword">/usr/</span>local/hadoop <span class="meta">#mv 实现重命名</span></span><br><span class="line">sudo chown -R hadoop:hadoop <span class="meta-keyword">/usr/</span>local/hadoop</span><br></pre></td></tr></table></figure><p>将当前的 PATH 环境变量提取保存到 P.sh</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">环境变量如下：</span><br><span class="line"></span><br><span class="line">export PATH=<span class="regexp">/usr/</span>local<span class="regexp">/sbin:/</span>usr<span class="regexp">/local/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">sbin:</span><span class="regexp">/usr/</span><span class="string">bin:</span><span class="regexp">/sbin:/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">games:</span><span class="regexp">/usr/</span>local<span class="regexp">/games:/</span>usr<span class="regexp">/local/</span>jvm<span class="regexp">/jdk1.8.0_60/</span><span class="string">bin:</span><span class="regexp">/usr/</span>local<span class="regexp">/jvm/</span>jdk1<span class="number">.8</span><span class="number">.0</span>_60<span class="regexp">/bin:/</span>usr<span class="regexp">/local/</span>hadoop<span class="regexp">/bin:/</span><span class="string">sbin:</span><span class="regexp">/usr/</span>local<span class="regexp">/sbin:/</span>usr<span class="regexp">/local/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">sbin:</span><span class="regexp">/usr/</span><span class="string">bin:</span><span class="regexp">/sbin:/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">games:</span><span class="regexp">/usr/</span>local<span class="regexp">/games:/</span>usr<span class="regexp">/local/</span>jvm<span class="regexp">/jdk1.8.0_60/</span><span class="string">bin:</span><span class="regexp">/usr/</span>local<span class="regexp">/hive/</span>bin</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=<span class="regexp">/usr/</span>local/hadoop</span><br><span class="line">export HIVE_HOME=<span class="regexp">/usr/</span>local/hive</span><br><span class="line">export JAVA_HOME=<span class="regexp">/usr/</span>local<span class="regexp">/jvm/</span>jdk1<span class="number">.8</span><span class="number">.0</span>_60</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;<span class="regexp">/lib:$&#123;JRE_HOME&#125;/</span><span class="string">lib:</span>$($HADOOP_HOME<span class="regexp">/bin/</span>hadoop classpath):$CLASSPATH</span><br><span class="line">export PATH=<span class="string">$PATH:</span>$&#123;JAVA_HOME&#125;<span class="regexp">/bin:$HADOOP_HOME/</span><span class="string">bin:</span>$HADOOP<span class="regexp">/sbin:$PATH:$&#123;HIVE_HOME&#125;/</span>bin</span><br></pre></td></tr></table></figure><p>之后每次重启系统都需要重定向环境变量</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">echo</span> $PATH &gt;&gt; ~/setenv.<span class="keyword">sh</span></span><br><span class="line"><span class="keyword">vi</span> ~/<span class="keyword">P</span>.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><p>使环境变量生效：<br></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> ~/setenv.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><p></p><p>查看java版本信息，如果出现版本信息则环境配置成功<br></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="built_in">version</span></span><br><span class="line">javac -<span class="built_in">version</span></span><br></pre></td></tr></table></figure><p></p><p><strong>重点：hadoop节点配置</strong></p><p>修改 slaves 文件，让 hadoop 知道自己可以聚合的节点名（保证与 hosts 里的角色<br>名一致）</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>slaves</span><br></pre></td></tr></table></figure><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">master</span></span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p>修改 core-site.xml 文件如下：<br>配置端口和路径<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>core-site.xml</span><br></pre></td></tr></table></figure><p></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改 hdfs-site.xml 文件如下（启用所有节点作为 DataNode，包括master故 replication_value=3）：<br>当节点增多时，需要更改配置文件，如主机名、IP地址、节点信息等配置都要重新修改</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span><span class="meta">?&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改 mapred-site.xml 文件如下：<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/m</span>apred-site.xml</span><br></pre></td></tr></table></figure><p></p><p>yarn为集群的表示<br></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p></p><p>修改 yarn-site.xml 文件如下（启用 yarn 资源管理器），为大数据计算分配计算、存储资源等<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/y</span>arn-site.xml</span><br></pre></td></tr></table></figure><p></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改 hadoop-env.sh 文件，将 25 行 JAVA_HOME 的值换成 jdk 所在的路径：<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>hadoop-env.sh</span><br></pre></td></tr></table></figure><p></p><p>当前jdk路径为：<code>JAVA_HOME=/usr/local/jvm/jdk1.8.0_60</code></p><p><strong>重点：hadoop启动及验证</strong></p><p>对 hadoop 进行 NameNode 的格式化：<br><code>/usr/local/hadoop/bin/hdfs namenode -format</code></p><p>启动 hdfs 和 yarn，并在各个节点上输入 jps 查看启动的服务：</p><p>只需在master上启动</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/hadoop/</span>sbin/start-dfs.sh</span><br><span class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/hadoop/</span>sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/hadoop/</span>sbin/start-all.sh</span><br><span class="line"></span><br><span class="line">jps <span class="meta"># 每个节点都查看一次</span></span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/2560767-ba2cf897a710d10a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="jps.png"></p><p>尝试在 hdfs 上创建输入文件夹input，并把 测试的txt文本文件放进去：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>loca<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -mkdir /i</span>nput</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -put /u</span>sr<span class="regexp">/local/</span>hadoop<span class="regexp">/a.txt /i</span>nput</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -put /u</span>sr<span class="regexp">/local/</span>hadoop<span class="regexp">/b.txt /i</span>nput</span><br></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">a</span>.txt</span><br><span class="line">Hello world</span><br><span class="line">bye hadoop</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">b</span>.txt</span><br><span class="line">bye world</span><br><span class="line">hello hadoop hadoop</span><br></pre></td></tr></table></figure><p>可以在外部浏览器输入 master 的 IP 地址和 50070 端口查看 hdfs 上的文件</p><p>输入：<code>http://192.168.142.128:50070</code></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-57aefff236e27576.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sumary-hadoop.png"></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-86f8934ecb41c6bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hadoopdatanode.png"></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-4ef3b55488645496.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hadoop-input.png"></p><p>尝试用 写一个简单的 WordCount.java代码，统计上面文本文件中每个单词<br>出现的频数：<br>hadoop的工作是把大任务切分成小jobs，再通过资源管理器分配存储和计算等资源给相应hadoop集群内的节点</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hadoop jar /u</span>sr<span class="regexp">/local/</span>hadoop<span class="regexp">/share/</span>hadoop<span class="regexp">/mapreduce/</span>hadoop-mapreduce-</span><br><span class="line">examples-<span class="number">2.6</span>.<span class="number">0</span>.jar wordcount <span class="regexp">/input /</span>output</span><br><span class="line"></span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -cat /</span>output<span class="regexp">/* #查看输出结果</span></span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/2560767-57019829b1015811.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hadoop-output.png"></p><p><strong>hadoop 搭建完成</strong></p><h3 id="三、Hive-数据仓库安装部署及测试"><a href="#三、Hive-数据仓库安装部署及测试" class="headerlink" title="三、Hive 数据仓库安装部署及测试"></a><strong>三、Hive 数据仓库安装部署及测试</strong></h3><p><strong>安装包准备</strong></p><p>apache-hive-1.2.0-bin.tar.gz</p><p>mysql-connector-java-5.1.44-bin.jar</p><p>testdata.zip</p><p>Hive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工<br>具，可以用来 进行数据提取转化加载（ETL） ），这是一种可以存储、查询和分析<br>存储在 Hadoop 中的大规模数据的机制。另外，它定义了简单的类 SQL 查询<br>语言，称为 HQL，允许熟悉 SQL 的用户查询数据。</p><p>如果是实施的交通数据，那么搭建动态的数据仓库是很有必要的。</p><p><strong>原理图</strong></p><p>Hive的工作原理简单来说就是一个查询引擎</p><p>Hive的架构图：<br><img src="http://upload-images.jianshu.io/upload_images/2560767-2d7a6e55039693be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hive架构.png"></p><p>Hive的工作原理如下：<br>接收到一个sql,后面做的事情包括：<br>1.词法分析/语法分析<br>使用antlr将SQL语句解析成抽象语法树-AST</p><p>2.语义分析<br>从Megastore获取模式信息，验证SQL语句中队表名,列名，以及数据类型的检查和隐式转换，以及Hive提供的函数和用户自定义的函数（UDF/UAF）</p><p>3.逻辑计划生产<br>生成逻辑计划-算子树</p><p>4.逻辑计划优化<br>对算子树进行优化，包括列剪枝，分区剪枝，谓词下推等</p><p>5.物理计划生成<br>将逻辑计划生产包含由MapReduce任务组成的DAG的物理计划</p><p>6.物理计划执行<br>将DAG发送到Hadoop集群进行执行</p><p>7.将查询结果返回</p><p>流程如下图:<br><img src="http://upload-images.jianshu.io/upload_images/2560767-082bb6fcae008cf6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hive流程.png"></p><p>hive和mapreduce共同运作</p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-b9db3ff25ec36371.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hive原理.png"></p><p><strong>用户接口</strong><br>CLI：就是 Shell 命令行.</p><p>JDBC：这个是 Hive 的 java 接口，与通常的数据库类似.</p><p>WebGUI：网页界面。</p><p><strong>驱动组件（Driver）</strong><br>Hive 的编译、解析、优化转化为 MapReduce 任务提交给 Hadoop 进行分派<br>和执行相应的任务。</p><p><strong>元数据组件（Metatore）</strong></p><p>存储着 hive 的元数据信息，包括表名、列、分区和属性等。默认数据库为<br>Derby，为了更健壮，一般使用 Mysql 来代替。另外，MetaStore 分为服务<br>端和客户端，服务端提供给客户端对 RDBMS 的访问服务。</p><p><strong>安装 Mysql</strong><br></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install mysql-server</span><br></pre></td></tr></table></figure><p></p><p>安装完成后设置数据库root用户的密码，<code>123456</code></p><p><strong>配置 Mysql</strong></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql –uroot –<span class="selector-tag">p</span> 的 使用初始化时设置的 root 密码登录</span><br></pre></td></tr></table></figure><p>新增 hive 用户，并给于权限：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">'hive'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'hive'</span>;</span><br><span class="line"><span class="keyword">grant</span> all <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'hive'</span> <span class="keyword">with</span> <span class="keyword">grant</span> <span class="keyword">option</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure><p><strong>Hive 安装与配置</strong></p><ol><li>安装 hive</li></ol><p>将 apache-hive-1.2.0-bin.tar.gz 解压在/usr/local 目录下</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar –zxvf apache-hive-<span class="number">1.2</span>.<span class="number">0</span>-bin<span class="selector-class">.tar</span><span class="selector-class">.gz</span> –C /usr/local</span><br></pre></td></tr></table></figure><p>重命名文件夹为 hive 文件夹，并将其权限修改成 hadoop</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv <span class="regexp">/usr/</span>local<span class="regexp">/ apache-hive-1.2.0-bin /</span>usr<span class="regexp">/local/</span>hive</span><br><span class="line">sudo chown -R <span class="string">hadoop:</span>hadoop <span class="regexp">/usr/</span>local/hive</span><br></pre></td></tr></table></figure><p>把 mysql 的 jdbc 的驱动 mysql-connector-java-5.1.44-bin.jar 拷贝到<br>\usr\local\hive\lib 目录下</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mysql-connector-java-<span class="number">5.1</span>.<span class="number">44</span>-bin.jar /usr/local/hive/<span class="class"><span class="keyword">lib</span></span></span><br></pre></td></tr></table></figure><ol><li>配置环境变量</li></ol><p>修改之前的P.sh</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">环境变量如下：</span><br><span class="line"></span><br><span class="line">export PATH=<span class="regexp">/usr/</span>local<span class="regexp">/sbin:/</span>usr<span class="regexp">/local/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">sbin:</span><span class="regexp">/usr/</span><span class="string">bin:</span><span class="regexp">/sbin:/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">games:</span><span class="regexp">/usr/</span>local<span class="regexp">/games:/</span>usr<span class="regexp">/local/</span>jvm<span class="regexp">/jdk1.8.0_60/</span><span class="string">bin:</span><span class="regexp">/usr/</span>local<span class="regexp">/jvm/</span>jdk1<span class="number">.8</span><span class="number">.0</span>_60<span class="regexp">/bin:/</span>usr<span class="regexp">/local/</span>hadoop<span class="regexp">/bin:/</span><span class="string">sbin:</span><span class="regexp">/usr/</span>local<span class="regexp">/sbin:/</span>usr<span class="regexp">/local/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">sbin:</span><span class="regexp">/usr/</span><span class="string">bin:</span><span class="regexp">/sbin:/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">games:</span><span class="regexp">/usr/</span>local<span class="regexp">/games:/</span>usr<span class="regexp">/local/</span>jvm<span class="regexp">/jdk1.8.0_60/</span><span class="string">bin:</span><span class="regexp">/usr/</span>local<span class="regexp">/hive/</span>bin</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=<span class="regexp">/usr/</span>local/hadoop</span><br><span class="line">export HIVE_HOME=<span class="regexp">/usr/</span>local/hive</span><br><span class="line">export JAVA_HOME=<span class="regexp">/usr/</span>local<span class="regexp">/jvm/</span>jdk1<span class="number">.8</span><span class="number">.0</span>_60</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;<span class="regexp">/lib:$&#123;JRE_HOME&#125;/</span><span class="string">lib:</span>$($HADOOP_HOME<span class="regexp">/bin/</span>hadoop classpath):$CLASSPATH</span><br><span class="line">export PATH=<span class="string">$PATH:</span>$&#123;JAVA_HOME&#125;<span class="regexp">/bin:$HADOOP_HOME/</span><span class="string">bin:</span>$HADOOP<span class="regexp">/sbin:$PATH:$&#123;HIVE_HOME&#125;/</span>bin</span><br></pre></td></tr></table></figure><p>修改完成后<code>source P.sh</code></p><ol><li>Hive 的配置</li></ol><p>hive 有关于 metastore 具有三种配置，分别为内嵌模式、本地元存储以及远程<br>在 元存储.</p><p>目前在 namenode 上配置服务端，datanode 上配置为客户端，在 datanode<br>行 进行 hql 时通过远程模式访问在 namenode 的元数据。（实际场景 metastore 的数据库可以在任何节点，以减轻 namenode 的压力）</p><p>在 /usr/local/hive/conf 目录下创建 hive-site.xml 文件：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi <span class="regexp">/usr/</span>local<span class="regexp">/hive/</span>conf<span class="regexp">/hive-site.xml</span></span><br></pre></td></tr></table></figure><p>在 server 端配置 hive-stie.xml,修改后保存</p><p>ConnectionURL 属性用于设置 mysql 服务所在地址与端口，这里 mysql-server 在 在<br>本地， hive.metastore.warehouse.dir 是在 HDFS 上的文件路径，hive.metastore.local 的 的<br>为 值为 true 表示对 metastore 的访问为本地模式。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span> standalone=<span class="string">"no"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span><span class="meta">?&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/db_hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /////////////////////////////////////////////////////////////////</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.142.128<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.schedulaer.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在 client 端配置 hive-stie.xml,修改后保存</p><p>hive.metastore.uris 项指向提供数据库访问的 metastore 服务端，值须为 IP 地 地<br>了 址。由于设置了 uris 的内容，因而对于 metastore 的访问默认为远程模式。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span> standalone=<span class="string">"no"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span><span class="meta">?&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.142.128:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	</span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li>运行 Hive</li></ol><p>检查 jline 版本，hive 与 hadoop 的 jline 版本不对应可能导致运行错误，<br>先删除之前的旧版本，然后将 hive 上 jline 的 jar 包拷贝至 hadoop 的对应目录下：</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/local/hive/<span class="class"><span class="keyword">lib</span>/<span class="title">jline</span>-2.12.<span class="title">jar</span> /<span class="title">usr</span>/<span class="title">local</span>/<span class="title">hadoop</span>/<span class="title">share</span>/<span class="title">hadoop</span>/<span class="title">yarn</span>/<span class="title">lib</span></span></span><br></pre></td></tr></table></figure><p>更新 yarn-site.xml 配置</p><p><strong>重要：为了使得 mapreduce 程序可以在各个节点提交，对各个节点的 hive-site.xml 配置文件 追加下 以下 property</strong></p><p>其中 yarn.resourcemanager.hostname 配置的是运行 ResourceManager 进程所在<br>的主机（master 节点）IP 地址。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.142.128<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.schedulaer.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>只需在master上进行：</p><p>启动hadoop：<code>/usr/local/hadoop/sbin/start-all.sh</code></p><p>初始化 Schema<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hive/</span>bin<span class="regexp">/./</span>schematool –dbType mysql –initSchema</span><br></pre></td></tr></table></figure><p></p><p>服务端启动 metastore 服务</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive <span class="comment">--service metastore</span></span><br></pre></td></tr></table></figure><p>接着在slave1和slave2启动hive的客户端</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hive</span></span><br></pre></td></tr></table></figure><ol><li>测试hive数据仓库：</li></ol><p>导入数据可以是使用Tab间隔的原数据形式,从本地导入。</p><p>也可以从 HDFS 中导入。与本地的类似，只要把本地载入的命令中’local’去掉，输入路径即为HDFS 上的路径。</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">list1.txt</span><br><span class="line"><span class="number">10001</span>	<span class="number">1003</span>	c2sld54sfkid	paul</span><br><span class="line"><span class="number">10002</span>	<span class="number">1014</span>	d5dsd32sdkif	rose</span><br><span class="line"><span class="number">10005</span>	<span class="number">1008</span>	e0dlp92mklpi	harry</span><br><span class="line"><span class="number">10203</span>	<span class="number">1099</span>	p3skd29llsie	bob</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">list2.txt</span><br><span class="line"><span class="number">20004</span>	<span class="number">2991</span>	e2ifp12adlpi	alice</span><br><span class="line"><span class="number">20132</span>	<span class="number">1014</span>	l8doo32haodp	jerry</span><br><span class="line"><span class="number">50232</span>	<span class="number">3022</span>	d3sod41dsooo	smith</span><br><span class="line"><span class="number">40001</span>	<span class="number">1023</span>	s3dfdlksdfj3	rose</span><br></pre></td></tr></table></figure><p>登入 hive 客户端，创建 user_info 表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">hive</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">create table user_info(uid int, did int, <span class="built_in">pwd</span> string, uname string)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">row format delimited</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">fields terminated by ‘\t’</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">lines terminated by ‘\n’;</span></span><br></pre></td></tr></table></figure><p>把数据加载到表:<br></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath ‘<span class="keyword">list</span>*.txt’ <span class="keyword">into</span> <span class="keyword">table</span> user_info;</span><br></pre></td></tr></table></figure><p></p><p>将查询结果插入到表</p><p>说明：将所有名为 rose 的记录插入到 account_rose 表中</p><p>先创建 account_rose 表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_info(uid <span class="built_in">int</span>,did <span class="built_in">int</span>,pwd <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> user_info;</span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/2560767-1c31a45ad9b9bca6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hive-result.png"></p><p><strong>Hive数据仓库建立成功</strong></p><p>动态数据仓库可以使用jsp进行动态的添加,结合Android、html或者其他可视化工具进行操作</p><h3 id="四、spark的配置和测试"><a href="#四、spark的配置和测试" class="headerlink" title="四、spark的配置和测试"></a><strong>四、spark的配置和测试</strong></h3><p>Spark 是专为大规模数据处理而设计的快速通<br>用的计算引擎，有与 hadoop 相似的开源集群计算环境，但是能在某些工作负载方面表现得<br>更加优越，换句话说，spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可<br>以优化迭代工作负载。<br>spark 有三种模式，分别是 standalone、spark on yarn，spark on mesos，这里我们选择 spark on<br>yarn 的模式，即在我们之前部署的 hadoop 集群上部署 spark。</p><blockquote><p>我的电脑是 1G 运行内存，所以设置的是 1000M，executor cores 是 2（经过测试 driver memory 和 worker。memory 如果小于 512M，executor cores 大于 2 的话用 spark 提交任务会很容易就崩死，提示的是 SparkContext 相关错误）。</p></blockquote><ol><li>将 spark-1.6.0-bin-hadoop2.6.tgz 放到/home/hadoop/目录下</li><li><p>解压文件到/usr/local 下，重命名文件夹并修改属主</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -xzvf spark<span class="number">-1.6</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>.tgz -C <span class="regexp">/usr/</span>local/</span><br><span class="line"></span><br><span class="line">sudo mv <span class="regexp">/usr/</span>local<span class="regexp">/spark-1.6.0-bin-hadoop2.6 /</span>usr<span class="regexp">/local/</span>spark</span><br><span class="line"></span><br><span class="line">sudo chown -R <span class="string">hadoop:</span>hadoop <span class="regexp">/usr/</span>local<span class="regexp">/spark/</span></span><br></pre></td></tr></table></figure></li><li><p>利用 spark 的 template 文件生成配置文件</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-env.<span class="keyword">sh</span>.template /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-env.<span class="keyword">sh</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cp /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/slaves.template /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/slaves</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cp /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-defaults.<span class="keyword">conf</span>.template /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-defaults.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure></li><li><p>修改 spark-env.sh，在文件末尾添加如下内容</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">HADOOP_HOME</span>=/usr/local/hadoop</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/lib/jvm/java-1.7.0-openjdk-amd64</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">HADOOP_CONF_DIR</span>=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_MASTER_IP</span>=master</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_LOCAL_DIRS</span>=/usr/local/spark</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_WORKER_MEMORY</span>=1000M</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_EXECUTOR_MEMORY</span>=1000M</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_DRIVER_MEMORY</span>=1000M</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_EXECUTOR_CORES</span>=3</span><br></pre></td></tr></table></figure></li></ol><p>这一步是为了配置spark的运行参数，hadoop_conf_dir的设置是为了让spark运行在yarn上。<br>几个 memory 命令分别用于设置 driver 和 executor 进程的内存，executor_cores 设置的是每个<br>executor 进程的 CPU cores 的数量，这些设置请依据自己的电脑实际可负载情况设置。</p><ol><li>修改 slaves 文件，在文件末尾添加其他节点 IP<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/spark/</span>conf<span class="regexp">/slaves</span></span><br></pre></td></tr></table></figure></li></ol><p>修改成<br></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">master</span></span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p></p><ol><li>修改 spark-defaults.conf，在文件末尾添加如下内容：<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-defaults.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure></li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark<span class="selector-class">.executor</span><span class="selector-class">.extraJavaOptions</span>  -XX:+PrintGCDetails -Dkey=value -Dnumbers=<span class="string">"one two three"</span></span><br><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.enabled</span>           true</span><br><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.dir</span>               hdfs:<span class="comment">//master:9000/historyserverforSpark</span></span><br><span class="line">spark<span class="selector-class">.yarn</span><span class="selector-class">.historyServer</span><span class="selector-class">.address</span> master:<span class="number">18080</span></span><br><span class="line">spark<span class="selector-class">.history</span><span class="selector-class">.fs</span><span class="selector-class">.logDirectory</span>    hdfs:<span class="comment">//master:9000/historyserverforSpark</span></span><br><span class="line">spark<span class="selector-class">.speculation</span>                true</span><br></pre></td></tr></table></figure><p>这一步是为保存 spark 的运行日志，并且是保存到 hdfs 上的文件夹里面，方便运维。</p><ol><li>将配置好的 spark 文件夹传到 slave1、slave2。</li></ol><ol><li>配置 hadoop:修改 yarn-site.xml 文件，添加新的属性。<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><p>这一步是为了开启日志整合功能，spark 在遇到问题时，利用命令<br>/usr/local/hadoop/bin/yarn logs -applicationId XXX<br>即可查看 applicationId 为 XXX 的实例的运行日志</p><p><strong>运行spark</strong></p><ol><li>开启hadoop集群</li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>sbin<span class="regexp">/start-all.sh</span></span><br></pre></td></tr></table></figure><ol><li>在 spark 中创建 historyserverforSpark 文件夹</li></ol><p>这个文件夹可以在网页上看见（hdfs）</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -mkdir historyserverforSpark</span></span><br></pre></td></tr></table></figure><ol><li>运行 spark<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/spark/</span>sbin<span class="regexp">/start-all.sh</span></span><br></pre></td></tr></table></figure></li></ol><p>可以进入 spark 的 webui 查看是否成功启动：<code>192.1468.142.128:8080/</code><br>可以进入 spark 的 webui 查看节点是否成功启动：<code>192.1468.142.128:8080/cluster</code></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-7b362e81b839dc9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sparkstartall.png"></p><ol><li>运行 history-server，这样应用运行完的结果可以通过 webui 看到:<code>192.1468.142.128:18080</code></li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usr<span class="regexp">/local/</span>spark<span class="regexp">/sbin/</span>start-history-server.sh</span><br></pre></td></tr></table></figure><p>spark运行历史</p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-f9a73d0140a2a540.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="spark"></p><p><strong>运行spark实例</strong></p><ol><li>本地模式</li></ol><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/spark/bin/<span class="keyword">run</span><span class="bash">-example SparkPi &gt; SparkPiRes.txt</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">vi SparkPiRes.txt</span></span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/2560767-bc2628e8c05111dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Piresult.png"></p><ol><li>在yarn集群上运行</li></ol><p>运行可以有client模式和cluster模式<br>区别如下</p><blockquote><p>When run SparkSubmit —class [mainClass], SparkSubmit will call a childMainClass which is</p><ol><li><p>client mode, childMainClass = mainClass</p></li><li><p>standalone cluster mde, childMainClass = org.apache.spark.deploy.Client</p></li><li><p>yarn cluster mode, childMainClass = org.apache.spark.deploy.yarn.Client</p></li></ol><p>The childMainClass is a wrapper of mainClass. The childMainClass will be called in SparkSubmit, and if cluster mode, the childMainClass will talk to the the cluster and launch a process on one woker to run the mainClass.</p><p>ps. use “spark-submit -v” to print debug infos.</p><p>Yarn client: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master yarn JavaWordCount.jar<br>childMainclass: org.apache.spark.examples.JavaWordCount<br>Yarn cluster: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master yarn-cluster JavaWordCount.jar<br>childMainclass: org.apache.spark.deploy.yarn.Client</p><p>Standalone client: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master spark://aa01:7077 JavaWordCount.jar<br>childMainclass: org.apache.spark.examples.JavaWordCount<br>Stanalone cluster: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master spark://aa01:7077 —deploy-mode cluster JavaWordCount.jar<br>childMainclass: org.apache.spark.deploy.rest.RestSubmissionClient (if rest, else org.apache.spark.deploy.Client)</p></blockquote><p>Taking standalone spark as example, here is the client mode workflow. The mainclass run in the driver application which could be reside out of the cluster.</p><p>client 模式结构<br><img src="http://upload-images.jianshu.io/upload_images/2560767-60c0f8ea5ea9b781.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="clientmode.jpg"></p><p>On cluster mode showed as below, SparkSubmit will register driver in the cluster, and a driver process launched in one work running the main class.</p><p>cluster模式结构<br><img src="http://upload-images.jianshu.io/upload_images/2560767-e8388857afce784f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="clustermode.jpg"></p><p>yarn-client模式可以在运行中看见输出日志。如果是使用 yarn-cluster 模式，在运行过程中不会看到统计结果。需要到日志文件中查看。由于cluster模式，分配资源的driver不在本节点，无法把结果输出到命令行，但是通常cluster模式速度更快一些。因为driver是任务提交后自行分配的，资源和job分配更加合理。</p><p>键入命令运行wordcount<br></p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">/usr/local/spark/bin/spark-submit</span> <span class="params">--deploy-mode</span> cluster <span class="params">--master</span> yarn <span class="params">--driver-memory</span> 1G <span class="params">--class</span> org.apache.spark.examples.JavaWordCount <span class="params">--executor-memory</span> 1G <span class="params">--total-executor-cores</span> 2 <span class="string">/usr/local/spark/lib/spark-examples-1.6.3-hadoop2.6.0.jar</span> hdfs:<span class="string">//master</span><span class="function">:9000</span>/input</span><br></pre></td></tr></table></figure><p></p><p>出现下图表示程序无错误，运行成功</p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-6525d52795b26ae3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="spark成功.png"></p><p>本次测试使用的也是单词的统计<br>输入如下<br></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">a</span>.txt</span><br><span class="line">Hello world</span><br><span class="line">bye hadoop</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">b</span>.txt</span><br><span class="line">bye world</span><br><span class="line">hello hadoop hadoop</span><br></pre></td></tr></table></figure><p></p><p>键入命令查看输出测试的输出结果<br><code>/usr/local/hadoop/bin/yarn logs -applicationId XXX</code></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-a753be3edc411cd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sparkwordcount.png"></p><h3 id="五、Scala与Sbt的配置和测试"><a href="#五、Scala与Sbt的配置和测试" class="headerlink" title="五、Scala与Sbt的配置和测试"></a><strong>五、Scala与Sbt的配置和测试</strong></h3><p>这篇博文解释得非常清楚~<a href="https://blog.csdn.net/a532672728/article/details/72477591" title="blog">https://blog.csdn.net/a532672728/article/details/72477591</a></p><h3 id="六、Item-based-Collaboration-Filtering"><a href="#六、Item-based-Collaboration-Filtering" class="headerlink" title="六、Item-based Collaboration Filtering"></a><strong>六、Item-based Collaboration Filtering</strong></h3><p>见另一篇博文</p><h3 id="七、SlopeOne"><a href="#七、SlopeOne" class="headerlink" title="七、SlopeOne"></a><strong>七、SlopeOne</strong></h3><p>见另一篇博文</p></div><div class="article-tags"><a class="tag-link" href="/tags/spark-hive-hadoop-scala/">spark,hive,hadoop,scala</a></div></article><script>document.getElementById("loading-bar").style.width="60%"</script></main><footer id="footer" class="clearfix"><div class="search"><form name="searchform" id="searchform" class="u-search-form"><input type="text" id="searchinput" class="u-search-input st-default-search-input" data-list-highlight="true" data-list-value-completion="true" placeholder="Looking for something?"> <button type="submit" id="u-search-btn-submit" class="u-search-btn-submit"><span class="icon icon-search"></span></button></form></div><div>&copy; <a href="http://zhongfuze.github.io">F泽泽馥泽泽杂记F</a> Theme by <a href="http://artifact.me/" target="_blank">Somebody</a>.</div><div>Powered by <a href="https://hexo.io/" rel="external">Hexo</a>.</div></footer><script>document.getElementById("loading-bar").style.width="80%"</script><div class="overlay"></div></div><div class="site-sidebar"><div class="sidebar-switch clearfix" style="display:none"><a class="dark-btn active" data-toggle="toc"><span class="icon icon-list"></span> <span class="text">Index</span> </a><a class="dark-btn" data-toggle="bio"><span class="icon icon-person"></span> <span class="text">Bio</span></a></div><div class="site-toc" style="display:none"><div class="no-index">No Index</div></div><div class="site-bio show" style="display:block"><div class="about-me clearfix"><div class="avatar"><img src="/img/avatar.png"></div><div class="info"><a class="name dark-btn" href="/about">ZhongFuze</a></div><div class="info"><span class="item desc">嘤嘤嘤</span></div></div><div class="social clearfix"><a href="https://space.bilibili.com/162740252/#/" class="bilibili" target="_blank" rel="external"><span class="icon icon-bilibili"></span> </a><a href="https://github.com/ZhongFuze" class="github" target="_blank" rel="external"><span class="icon icon-github"></span></a></div><div class="shortcuts clearfix"><div class="bk"><a href="#header" class="dark-btn window-nav"><span class="icon icon-chevron-thin-up"></span> <span class="text">Back to Top</span></a></div><div class="bk"><a href="#footer" class="dark-btn window-nav"><span class="icon icon-chevron-thin-down"></span> <span class="text">Go to Bottom</span></a></div></div></div></div><script type="text/javascript">var GOOGLE_CUSTOM_SEARCH_API_KEY="",GOOGLE_CUSTOM_SEARCH_ENGINE_ID="",ALGOLIA_API_KEY="",ALGOLIA_APP_ID="",ALGOLIA_INDEX_NAME="",AZURE_SERVICE_NAME="",AZURE_INDEX_NAME="",AZURE_QUERY_KEY="",BAIDU_API_ID="",SEARCH_SERVICE="hexo"</script><script src="https://code.jquery.com/jquery-2.1.4.min.js"></script><script>window.jQuery||document.write('<script src="/js/jquery.js"><\/script>')</script><script src="/js/search.js"></script><script src="/js/app.js"></script><script>document.getElementById("loading-bar").style.width="100%"</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>