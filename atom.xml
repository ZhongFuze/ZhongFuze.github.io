<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>F泽泽馥泽泽杂记F</title>
  
  <subtitle>Hello World</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhongfuze.github.io/"/>
  <updated>2018-08-10T08:09:39.290Z</updated>
  <id>http://zhongfuze.github.io/</id>
  
  <author>
    <name>ZhongFuze</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Plantform</title>
    <link href="http://zhongfuze.github.io/2018/08/10/Plantform/"/>
    <id>http://zhongfuze.github.io/2018/08/10/Plantform/</id>
    <published>2018-08-10T07:43:31.000Z</published>
    <updated>2018-08-10T08:09:39.290Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Mapreduce-Hive-Spark平台搭建"><a href="#Mapreduce-Hive-Spark平台搭建" class="headerlink" title="Mapreduce,Hive,Spark平台搭建"></a>Mapreduce,Hive,Spark平台搭建</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>平台搭建成功后，使用Scala语言进行算法设计和应用的开发。文末有使用scala设计的Item-Based Collaboration Filtering和SlopeOne 两个协同过滤算法</p><h3 id="一、搭建准备"><a href="#一、搭建准备" class="headerlink" title="一、搭建准备"></a><strong>一、搭建准备</strong></h3><p>VMWare Workstation</p><p>Ubuntu 14.04 Server.iso</p><p>Xshell——远程连接主机终端</p><p><strong>Server 版安装配置</strong><br>新建三台虚拟机，安装时选择OpenSHH Server预安装环境</p><p>一台作为master</p><p>另两台作为slave，命名为slave1和slave2</p><p>master的配置比slave要稍微高一些</p><p><strong>IP 设置</strong><br>VMnet8是VM配置的虚拟网卡</p><p>设置VMnet8，选择「NAT 模式」</p><p>设置子网IP （192.168.142.0）和子网掩码（255.255.255.0）</p><p><strong>Ubuntu 系统的登录和 IP 的固化</strong></p><p>输入 <code>ifconfig</code> 回车查询当前的 ip 地址，准备将其长期绑定到当前虚拟机,为了方便后续实验。</p><blockquote><p>master的ip：192.168.142.128</p><p>slave1的ip：192.168.142.130</p><p>slave2的ip：192.168.142.132</p></blockquote><p>此时的 IP 地址是由 DHCP 服务器动态分配的，为了让这个 IP 地址能一直与这台<br>虚拟机绑定，我们需要改变系统启动时获取 IP 的方式，从 DHCP 切换到静态 IP<br>地址，为此需要编辑 Linux 的网卡配置文件（/etc/network/interfaces），输入命令</p><p><code>sudo vi /etc/network/interfaces</code></p><p>回车，可以看到 eth0 的 IP 获取方式是 dhcp：</p><p>此时需要修改 ip 的获取方式从 DHCP 到 static，设置静态的 ip 地址、子网掩码和<br>默认网关。</p><p>把这个文件更改为：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#This file describes the network interfaces available on your system</span></span><br><span class="line"><span class="comment"># and how to activate them. For more information, see interfaces(5).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The loopback network interface</span></span><br><span class="line">auto lo</span><br><span class="line">iface lo inet loopback</span><br><span class="line"></span><br><span class="line"><span class="comment"># The primary network interface</span></span><br><span class="line">auto eth0</span><br><span class="line">iface eth0 inet static</span><br><span class="line">address 192.168.142.128</span><br><span class="line">gateway 192.168.142.2</span><br><span class="line">netmask 255.255.255.0</span><br></pre></td></tr></table></figure><p>为了进一步固化 DNS 服务器 IP，输入</p><p><code>sudo vi /etc/resolvconf/resolv.conf.d/bas</code></p><p>将里面的内容替换成<code>nameserver 192.168.142.2</code>，vi 保存</p><p><strong>系统的登录和 IP 的固化重启后完成</strong></p><p><strong>使用Xshell连接三个虚拟机终端</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-f6a9ce17b3444f5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Xshell.png"></p><p><strong>下载lrzsz工具</strong><br>用于虚拟机与宿主机之间文件的上传和下载</p><p><code>sudo apt-get update</code></p><p><code>sudo apt-get install lrzsz</code></p><p>rz命令，从宿主机传文件至虚拟机</p><p>sz命令，从虚拟机获取文件至宿主机</p><h3 id="二、Hadoop-及相关环境的安装"><a href="#二、Hadoop-及相关环境的安装" class="headerlink" title="二、Hadoop 及相关环境的安装"></a><strong>二、Hadoop 及相关环境的安装</strong></h3><p>开源分布式平台 Hadoop 可以聚合多个计算机形成集群，在各个节点上安装配置完Hadoop后可以直接提交分布式代码到集群计算。本次实验可以在个人电脑上用VMware完成，或使用天河二号上的 OpenStack平台创建内存为2G的虚拟机资源完成。</p><p><strong>安装包准备</strong></p><p>Hadoop环境——hadoop-2.6.0.tar.gz</p><p>Java环境——jdk-8u60-linux-x64.tar.gz</p><p><strong>虚拟机ip配置测试</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/hosts #编辑 /etc/hosts 文件，插入角色与<span class="built_in"> IP </span>映射</span><br><span class="line"></span><br><span class="line">ping master -c 4 #尝试用角色名<span class="built_in"> ping </span>其它主机，一次 4 个包</span><br></pre></td></tr></table></figure><p>hosts文件修改为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">127<span class="selector-class">.0</span><span class="selector-class">.0</span><span class="selector-class">.1</span>       <span class="selector-tag">localhost</span></span><br><span class="line"></span><br><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.142</span><span class="selector-class">.128</span> <span class="selector-tag">master</span></span><br><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.142</span><span class="selector-class">.130</span> <span class="selector-tag">slave1</span></span><br><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.142</span><span class="selector-class">.132</span> <span class="selector-tag">slave2</span></span><br><span class="line"></span><br><span class="line"># <span class="selector-tag">The</span> <span class="selector-tag">following</span> <span class="selector-tag">lines</span> <span class="selector-tag">are</span> <span class="selector-tag">desirable</span> <span class="selector-tag">for</span> <span class="selector-tag">IPv6</span> <span class="selector-tag">capable</span> <span class="selector-tag">hosts</span></span><br><span class="line"><span class="selector-pseudo">::1</span>     <span class="selector-tag">localhost</span> <span class="selector-tag">ip6-localhost</span> <span class="selector-tag">ip6-loopback</span></span><br><span class="line"><span class="selector-tag">ff02</span><span class="selector-pseudo">::1</span> <span class="selector-tag">ip6-allnodes</span></span><br><span class="line"><span class="selector-tag">ff02</span><span class="selector-pseudo">::2</span> <span class="selector-tag">ip6-allrouters</span></span><br></pre></td></tr></table></figure><p>三个虚拟机能够使用主机名（不是ip）ping通即配置正确</p><p><strong>配置 SSH 无密码登录</strong><br>保障了 Hadoop 可以通过角色名在局域网里找到各个节点，为了让 Hadoop 可以进<br>一步读取、操作各个节点，需要赋予其登录的权限，意即让 Hadoop 拥有各个节点的普通用户账<br>号，从而在需要操作各个节点时直接用对应的账号登录获取操作权限。SSH 协议可以为节点上的<br>账户创建唯一的公私钥，然后利用这些公私钥实现无密码登录，从而让 Hadoop 直接绕开传统的<br>账号密码登录过程，直接用公私钥访问节点。</p><p>生成各个节点的 SSH 公私钥：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> ~<span class="string">/.ssh</span> <span class="comment"># 如果没有该目录，先执行一次 ssh localhost</span></span><br><span class="line">rm <span class="string">./id_rsa</span>* <span class="comment"># 删除之前生成的公匙（如果有）</span></span><br><span class="line">ssh-keygen -t rsa <span class="comment"># 一直按回车就可以</span></span><br></pre></td></tr></table></figure><p>为了让每个节点都拥有其它节点的公钥，要先把所有公钥放进一个文件里</p><ol><li><p>在 master 上，将 master 的公钥复制到 authorized_keys 文件里：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ./id_rsa.pub <span class="meta">&gt;&gt; </span>./authorized_keys <span class="comment"># cat 命令用于提取内容，&gt;&gt;输出重定向</span></span><br></pre></td></tr></table></figure></li><li><p>将 slave1、slave2 的公钥文件发送给 master，此时的传送要输入密码：</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp ~<span class="regexp">/.ssh/id</span>_rsa.pub hadoop<span class="variable">@master</span><span class="symbol">:/home/hadoop/</span></span><br></pre></td></tr></table></figure></li><li><p>Master 将接收到的 slave1 的公钥文件里的内容提取追加到 authorized_keys 文<br>件里：</p></li></ol><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~<span class="string">/id_rsa.pub</span> &gt;&gt; ~<span class="string">/.ssh/authorized_keys</span></span><br></pre></td></tr></table></figure><ol><li>将 slave2 的公钥内容也放进 authorized_keys 文件，然后将<br>authorized_keys 文件分别发送到两个 slave 的~/.ssh/下：<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp ~<span class="regexp">/.ssh/authorized</span>_keys hadoop<span class="variable">@slave1</span><span class="symbol">:/home/hadoop/</span>.ssh/</span><br></pre></td></tr></table></figure></li></ol><p>搭建成功表现：每个节点尝试使用 ssh &lt;角色名&gt;的命令直接登录其它节点，直到每个节点都可以<br>成功免密码登录其它节点，则免密码登录配置成功！ 如在 master 上输入：<code>ssh slave1</code>即可直接登陆slave1的虚拟机，不需要再输入密码登陆，便于后续实验操作。</p><p><strong>配置 Java环境和Hadoop环境</strong></p><p>安装包准备：<br>jdk-8u60-linux-x64.tar</p><p>hadoop-2.6.0.tar.gz</p><ol><li><p>安装 JDK<br>将上传的 JDK 压缩包（jdk-8u60-linux-x64.tar）放到根目录<code>/home/hadoop/</code>，解<br>压并放到指定的文件夹：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -<span class="selector-tag">p</span> /usr/local/jvm</span><br><span class="line">tar -zxvf jdk-<span class="number">8</span>u60-linux-x64<span class="selector-class">.tar</span><span class="selector-class">.gz</span> -C /usr/local/jvm</span><br></pre></td></tr></table></figure></li><li><p>安装 Hadoop</p></li></ol><p>在各个节点上将 hadoop 解压到<code>/usr/local/</code>目录下，改变其所属用户和所属组（让<br>hadoop 软件用 hadoop 账号登录时对 hadoop 文件夹拥有最高权限）：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop<span class="number">-2.6</span><span class="number">.0</span>.tar.gz -C <span class="meta-keyword">/usr/</span>local/</span><br><span class="line">sudo mv <span class="meta-keyword">/usr/</span>loca/hadoop<span class="number">-2.6</span><span class="number">.0</span> <span class="meta-keyword">/usr/</span>local/hadoop <span class="meta">#mv 实现重命名</span></span><br><span class="line">sudo chown -R hadoop:hadoop <span class="meta-keyword">/usr/</span>local/hadoop</span><br></pre></td></tr></table></figure><p>将当前的 PATH 环境变量提取保存到 P.sh</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">环境变量如下：</span><br><span class="line"></span><br><span class="line">export PATH=<span class="regexp">/usr/</span>local<span class="regexp">/sbin:/</span>usr<span class="regexp">/local/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">sbin:</span><span class="regexp">/usr/</span><span class="string">bin:</span><span class="regexp">/sbin:/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">games:</span><span class="regexp">/usr/</span>local<span class="regexp">/games:/</span>usr<span class="regexp">/local/</span>jvm<span class="regexp">/jdk1.8.0_60/</span><span class="string">bin:</span><span class="regexp">/usr/</span>local<span class="regexp">/jvm/</span>jdk1<span class="number">.8</span><span class="number">.0</span>_60<span class="regexp">/bin:/</span>usr<span class="regexp">/local/</span>hadoop<span class="regexp">/bin:/</span><span class="string">sbin:</span><span class="regexp">/usr/</span>local<span class="regexp">/sbin:/</span>usr<span class="regexp">/local/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">sbin:</span><span class="regexp">/usr/</span><span class="string">bin:</span><span class="regexp">/sbin:/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">games:</span><span class="regexp">/usr/</span>local<span class="regexp">/games:/</span>usr<span class="regexp">/local/</span>jvm<span class="regexp">/jdk1.8.0_60/</span><span class="string">bin:</span><span class="regexp">/usr/</span>local<span class="regexp">/hive/</span>bin</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=<span class="regexp">/usr/</span>local/hadoop</span><br><span class="line">export HIVE_HOME=<span class="regexp">/usr/</span>local/hive</span><br><span class="line">export JAVA_HOME=<span class="regexp">/usr/</span>local<span class="regexp">/jvm/</span>jdk1<span class="number">.8</span><span class="number">.0</span>_60</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;<span class="regexp">/lib:$&#123;JRE_HOME&#125;/</span><span class="string">lib:</span>$($HADOOP_HOME<span class="regexp">/bin/</span>hadoop classpath):$CLASSPATH</span><br><span class="line">export PATH=<span class="string">$PATH:</span>$&#123;JAVA_HOME&#125;<span class="regexp">/bin:$HADOOP_HOME/</span><span class="string">bin:</span>$HADOOP<span class="regexp">/sbin:$PATH:$&#123;HIVE_HOME&#125;/</span>bin</span><br></pre></td></tr></table></figure><p>之后每次重启系统都需要重定向环境变量</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">echo</span> $PATH &gt;&gt; ~/setenv.<span class="keyword">sh</span></span><br><span class="line"><span class="keyword">vi</span> ~/<span class="keyword">P</span>.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><p>使环境变量生效：<br></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> ~/setenv.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><p></p><p>查看java版本信息，如果出现版本信息则环境配置成功<br></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="built_in">version</span></span><br><span class="line">javac -<span class="built_in">version</span></span><br></pre></td></tr></table></figure><p></p><p><strong>重点：hadoop节点配置</strong></p><p>修改 slaves 文件，让 hadoop 知道自己可以聚合的节点名（保证与 hosts 里的角色<br>名一致）</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>slaves</span><br></pre></td></tr></table></figure><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">master</span></span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p>修改 core-site.xml 文件如下：<br>配置端口和路径<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>core-site.xml</span><br></pre></td></tr></table></figure><p></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改 hdfs-site.xml 文件如下（启用所有节点作为 DataNode，包括master故 replication_value=3）：<br>当节点增多时，需要更改配置文件，如主机名、IP地址、节点信息等配置都要重新修改</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span><span class="meta">?&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改 mapred-site.xml 文件如下：<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/m</span>apred-site.xml</span><br></pre></td></tr></table></figure><p></p><p>yarn为集群的表示<br></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p></p><p>修改 yarn-site.xml 文件如下（启用 yarn 资源管理器），为大数据计算分配计算、存储资源等<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/y</span>arn-site.xml</span><br></pre></td></tr></table></figure><p></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改 hadoop-env.sh 文件，将 25 行 JAVA_HOME 的值换成 jdk 所在的路径：<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>hadoop-env.sh</span><br></pre></td></tr></table></figure><p></p><p>当前jdk路径为：<code>JAVA_HOME=/usr/local/jvm/jdk1.8.0_60</code></p><p><strong>重点：hadoop启动及验证</strong></p><p>对 hadoop 进行 NameNode 的格式化：<br><code>/usr/local/hadoop/bin/hdfs namenode -format</code></p><p>启动 hdfs 和 yarn，并在各个节点上输入 jps 查看启动的服务：</p><p>只需在master上启动</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/hadoop/</span>sbin/start-dfs.sh</span><br><span class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/hadoop/</span>sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/hadoop/</span>sbin/start-all.sh</span><br><span class="line"></span><br><span class="line">jps <span class="meta"># 每个节点都查看一次</span></span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/2560767-ba2cf897a710d10a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="jps.png"></p><p>尝试在 hdfs 上创建输入文件夹input，并把 测试的txt文本文件放进去：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>loca<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -mkdir /i</span>nput</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -put /u</span>sr<span class="regexp">/local/</span>hadoop<span class="regexp">/a.txt /i</span>nput</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -put /u</span>sr<span class="regexp">/local/</span>hadoop<span class="regexp">/b.txt /i</span>nput</span><br></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">a</span>.txt</span><br><span class="line">Hello world</span><br><span class="line">bye hadoop</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">b</span>.txt</span><br><span class="line">bye world</span><br><span class="line">hello hadoop hadoop</span><br></pre></td></tr></table></figure><p>可以在外部浏览器输入 master 的 IP 地址和 50070 端口查看 hdfs 上的文件</p><p>输入：<code>http://192.168.142.128:50070</code></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-57aefff236e27576.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sumary-hadoop.png"></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-86f8934ecb41c6bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hadoopdatanode.png"></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-4ef3b55488645496.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hadoop-input.png"></p><p>尝试用 写一个简单的 WordCount.java代码，统计上面文本文件中每个单词<br>出现的频数：<br>hadoop的工作是把大任务切分成小jobs，再通过资源管理器分配存储和计算等资源给相应hadoop集群内的节点</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hadoop jar /u</span>sr<span class="regexp">/local/</span>hadoop<span class="regexp">/share/</span>hadoop<span class="regexp">/mapreduce/</span>hadoop-mapreduce-</span><br><span class="line">examples-<span class="number">2.6</span>.<span class="number">0</span>.jar wordcount <span class="regexp">/input /</span>output</span><br><span class="line"></span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -cat /</span>output<span class="regexp">/* #查看输出结果</span></span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/2560767-57019829b1015811.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hadoop-output.png"></p><p><strong>hadoop 搭建完成</strong></p><h3 id="三、Hive-数据仓库安装部署及测试"><a href="#三、Hive-数据仓库安装部署及测试" class="headerlink" title="三、Hive 数据仓库安装部署及测试"></a><strong>三、Hive 数据仓库安装部署及测试</strong></h3><p><strong>安装包准备</strong></p><p>apache-hive-1.2.0-bin.tar.gz</p><p>mysql-connector-java-5.1.44-bin.jar</p><p>testdata.zip</p><p>Hive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工<br>具，可以用来 进行数据提取转化加载（ETL） ），这是一种可以存储、查询和分析<br>存储在 Hadoop 中的大规模数据的机制。另外，它定义了简单的类 SQL 查询<br>语言，称为 HQL，允许熟悉 SQL 的用户查询数据。</p><p>如果是实施的交通数据，那么搭建动态的数据仓库是很有必要的。</p><p><strong>原理图</strong></p><p>Hive的工作原理简单来说就是一个查询引擎</p><p>Hive的架构图：<br><img src="http://upload-images.jianshu.io/upload_images/2560767-2d7a6e55039693be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hive架构.png"></p><p>Hive的工作原理如下：<br>接收到一个sql,后面做的事情包括：<br>1.词法分析/语法分析<br>使用antlr将SQL语句解析成抽象语法树-AST</p><p>2.语义分析<br>从Megastore获取模式信息，验证SQL语句中队表名,列名，以及数据类型的检查和隐式转换，以及Hive提供的函数和用户自定义的函数（UDF/UAF）</p><p>3.逻辑计划生产<br>生成逻辑计划-算子树</p><p>4.逻辑计划优化<br>对算子树进行优化，包括列剪枝，分区剪枝，谓词下推等</p><p>5.物理计划生成<br>将逻辑计划生产包含由MapReduce任务组成的DAG的物理计划</p><p>6.物理计划执行<br>将DAG发送到Hadoop集群进行执行</p><p>7.将查询结果返回</p><p>流程如下图:<br><img src="http://upload-images.jianshu.io/upload_images/2560767-082bb6fcae008cf6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hive流程.png"></p><p>hive和mapreduce共同运作</p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-b9db3ff25ec36371.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hive原理.png"></p><p><strong>用户接口</strong><br>CLI：就是 Shell 命令行.</p><p>JDBC：这个是 Hive 的 java 接口，与通常的数据库类似.</p><p>WebGUI：网页界面。</p><p><strong>驱动组件（Driver）</strong><br>Hive 的编译、解析、优化转化为 MapReduce 任务提交给 Hadoop 进行分派<br>和执行相应的任务。</p><p><strong>元数据组件（Metatore）</strong></p><p>存储着 hive 的元数据信息，包括表名、列、分区和属性等。默认数据库为<br>Derby，为了更健壮，一般使用 Mysql 来代替。另外，MetaStore 分为服务<br>端和客户端，服务端提供给客户端对 RDBMS 的访问服务。</p><p><strong>安装 Mysql</strong><br></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install mysql-server</span><br></pre></td></tr></table></figure><p></p><p>安装完成后设置数据库root用户的密码，<code>123456</code></p><p><strong>配置 Mysql</strong></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql –uroot –<span class="selector-tag">p</span> 的 使用初始化时设置的 root 密码登录</span><br></pre></td></tr></table></figure><p>新增 hive 用户，并给于权限：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">'hive'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'hive'</span>;</span><br><span class="line"><span class="keyword">grant</span> all <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'hive'</span> <span class="keyword">with</span> <span class="keyword">grant</span> <span class="keyword">option</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure><p><strong>Hive 安装与配置</strong></p><ol><li>安装 hive</li></ol><p>将 apache-hive-1.2.0-bin.tar.gz 解压在/usr/local 目录下</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar –zxvf apache-hive-<span class="number">1.2</span>.<span class="number">0</span>-bin<span class="selector-class">.tar</span><span class="selector-class">.gz</span> –C /usr/local</span><br></pre></td></tr></table></figure><p>重命名文件夹为 hive 文件夹，并将其权限修改成 hadoop</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv <span class="regexp">/usr/</span>local<span class="regexp">/ apache-hive-1.2.0-bin /</span>usr<span class="regexp">/local/</span>hive</span><br><span class="line">sudo chown -R <span class="string">hadoop:</span>hadoop <span class="regexp">/usr/</span>local/hive</span><br></pre></td></tr></table></figure><p>把 mysql 的 jdbc 的驱动 mysql-connector-java-5.1.44-bin.jar 拷贝到<br>\usr\local\hive\lib 目录下</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mysql-connector-java-<span class="number">5.1</span>.<span class="number">44</span>-bin.jar /usr/local/hive/<span class="class"><span class="keyword">lib</span></span></span><br></pre></td></tr></table></figure><ol><li>配置环境变量</li></ol><p>修改之前的P.sh</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">环境变量如下：</span><br><span class="line"></span><br><span class="line">export PATH=<span class="regexp">/usr/</span>local<span class="regexp">/sbin:/</span>usr<span class="regexp">/local/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">sbin:</span><span class="regexp">/usr/</span><span class="string">bin:</span><span class="regexp">/sbin:/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">games:</span><span class="regexp">/usr/</span>local<span class="regexp">/games:/</span>usr<span class="regexp">/local/</span>jvm<span class="regexp">/jdk1.8.0_60/</span><span class="string">bin:</span><span class="regexp">/usr/</span>local<span class="regexp">/jvm/</span>jdk1<span class="number">.8</span><span class="number">.0</span>_60<span class="regexp">/bin:/</span>usr<span class="regexp">/local/</span>hadoop<span class="regexp">/bin:/</span><span class="string">sbin:</span><span class="regexp">/usr/</span>local<span class="regexp">/sbin:/</span>usr<span class="regexp">/local/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">sbin:</span><span class="regexp">/usr/</span><span class="string">bin:</span><span class="regexp">/sbin:/</span><span class="string">bin:</span><span class="regexp">/usr/</span><span class="string">games:</span><span class="regexp">/usr/</span>local<span class="regexp">/games:/</span>usr<span class="regexp">/local/</span>jvm<span class="regexp">/jdk1.8.0_60/</span><span class="string">bin:</span><span class="regexp">/usr/</span>local<span class="regexp">/hive/</span>bin</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=<span class="regexp">/usr/</span>local/hadoop</span><br><span class="line">export HIVE_HOME=<span class="regexp">/usr/</span>local/hive</span><br><span class="line">export JAVA_HOME=<span class="regexp">/usr/</span>local<span class="regexp">/jvm/</span>jdk1<span class="number">.8</span><span class="number">.0</span>_60</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;<span class="regexp">/lib:$&#123;JRE_HOME&#125;/</span><span class="string">lib:</span>$($HADOOP_HOME<span class="regexp">/bin/</span>hadoop classpath):$CLASSPATH</span><br><span class="line">export PATH=<span class="string">$PATH:</span>$&#123;JAVA_HOME&#125;<span class="regexp">/bin:$HADOOP_HOME/</span><span class="string">bin:</span>$HADOOP<span class="regexp">/sbin:$PATH:$&#123;HIVE_HOME&#125;/</span>bin</span><br></pre></td></tr></table></figure><p>修改完成后<code>source P.sh</code></p><ol><li>Hive 的配置</li></ol><p>hive 有关于 metastore 具有三种配置，分别为内嵌模式、本地元存储以及远程<br>在 元存储.</p><p>目前在 namenode 上配置服务端，datanode 上配置为客户端，在 datanode<br>行 进行 hql 时通过远程模式访问在 namenode 的元数据。（实际场景 metastore 的数据库可以在任何节点，以减轻 namenode 的压力）</p><p>在 /usr/local/hive/conf 目录下创建 hive-site.xml 文件：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi <span class="regexp">/usr/</span>local<span class="regexp">/hive/</span>conf<span class="regexp">/hive-site.xml</span></span><br></pre></td></tr></table></figure><p>在 server 端配置 hive-stie.xml,修改后保存</p><p>ConnectionURL 属性用于设置 mysql 服务所在地址与端口，这里 mysql-server 在 在<br>本地， hive.metastore.warehouse.dir 是在 HDFS 上的文件路径，hive.metastore.local 的 的<br>为 值为 true 表示对 metastore 的访问为本地模式。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span> standalone=<span class="string">"no"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span><span class="meta">?&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/db_hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /////////////////////////////////////////////////////////////////</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.142.128<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.schedulaer.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在 client 端配置 hive-stie.xml,修改后保存</p><p>hive.metastore.uris 项指向提供数据库访问的 metastore 服务端，值须为 IP 地 地<br>了 址。由于设置了 uris 的内容，因而对于 metastore 的访问默认为远程模式。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span> standalone=<span class="string">"no"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span><span class="meta">?&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.142.128:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li>运行 Hive</li></ol><p>检查 jline 版本，hive 与 hadoop 的 jline 版本不对应可能导致运行错误，<br>先删除之前的旧版本，然后将 hive 上 jline 的 jar 包拷贝至 hadoop 的对应目录下：</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/local/hive/<span class="class"><span class="keyword">lib</span>/<span class="title">jline</span>-2.12.<span class="title">jar</span> /<span class="title">usr</span>/<span class="title">local</span>/<span class="title">hadoop</span>/<span class="title">share</span>/<span class="title">hadoop</span>/<span class="title">yarn</span>/<span class="title">lib</span></span></span><br></pre></td></tr></table></figure><p>更新 yarn-site.xml 配置</p><p><strong>重要：为了使得 mapreduce 程序可以在各个节点提交，对各个节点的 hive-site.xml 配置文件 追加下 以下 property</strong></p><p>其中 yarn.resourcemanager.hostname 配置的是运行 ResourceManager 进程所在<br>的主机（master 节点）IP 地址。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.142.128<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.schedulaer.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>只需在master上进行：</p><p>启动hadoop：<code>/usr/local/hadoop/sbin/start-all.sh</code></p><p>初始化 Schema<br></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hive/</span>bin<span class="regexp">/./</span>schematool –dbType mysql –initSchema</span><br></pre></td></tr></table></figure><p></p><p>服务端启动 metastore 服务</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive <span class="comment">--service metastore</span></span><br></pre></td></tr></table></figure><p>接着在slave1和slave2启动hive的客户端</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hive</span></span><br></pre></td></tr></table></figure><ol><li>测试hive数据仓库：</li></ol><p>导入数据可以是使用Tab间隔的原数据形式,从本地导入。</p><p>也可以从 HDFS 中导入。与本地的类似，只要把本地载入的命令中’local’去掉，输入路径即为HDFS 上的路径。</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">list1.txt</span><br><span class="line"><span class="number">10001</span><span class="number">1003</span>c2sld54sfkidpaul</span><br><span class="line"><span class="number">10002</span><span class="number">1014</span>d5dsd32sdkifrose</span><br><span class="line"><span class="number">10005</span><span class="number">1008</span>e0dlp92mklpiharry</span><br><span class="line"><span class="number">10203</span><span class="number">1099</span>p3skd29llsiebob</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">list2.txt</span><br><span class="line"><span class="number">20004</span><span class="number">2991</span>e2ifp12adlpialice</span><br><span class="line"><span class="number">20132</span><span class="number">1014</span>l8doo32haodpjerry</span><br><span class="line"><span class="number">50232</span><span class="number">3022</span>d3sod41dsooosmith</span><br><span class="line"><span class="number">40001</span><span class="number">1023</span>s3dfdlksdfj3rose</span><br></pre></td></tr></table></figure><p>登入 hive 客户端，创建 user_info 表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">hive</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">create table user_info(uid int, did int, <span class="built_in">pwd</span> string, uname string)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">row format delimited</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">fields terminated by ‘\t’</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">lines terminated by ‘\n’;</span></span><br></pre></td></tr></table></figure><p>把数据加载到表:<br></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath ‘<span class="keyword">list</span>*.txt’ <span class="keyword">into</span> <span class="keyword">table</span> user_info;</span><br></pre></td></tr></table></figure><p></p><p>将查询结果插入到表</p><p>说明：将所有名为 rose 的记录插入到 account_rose 表中</p><p>先创建 account_rose 表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_info(uid <span class="built_in">int</span>,did <span class="built_in">int</span>,pwd <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> user_info;</span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/2560767-1c31a45ad9b9bca6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hive-result.png"></p><p><strong>Hive数据仓库建立成功</strong></p><p>动态数据仓库可以使用jsp进行动态的添加,结合Android、html或者其他可视化工具进行操作</p><h3 id="四、spark的配置和测试"><a href="#四、spark的配置和测试" class="headerlink" title="四、spark的配置和测试"></a><strong>四、spark的配置和测试</strong></h3><p>Spark 是专为大规模数据处理而设计的快速通<br>用的计算引擎，有与 hadoop 相似的开源集群计算环境，但是能在某些工作负载方面表现得<br>更加优越，换句话说，spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可<br>以优化迭代工作负载。<br>spark 有三种模式，分别是 standalone、spark on yarn，spark on mesos，这里我们选择 spark on<br>yarn 的模式，即在我们之前部署的 hadoop 集群上部署 spark。</p><blockquote><p>我的电脑是 1G 运行内存，所以设置的是 1000M，executor cores 是 2（经过测试 driver memory 和 worker。memory 如果小于 512M，executor cores 大于 2 的话用 spark 提交任务会很容易就崩死，提示的是 SparkContext 相关错误）。</p></blockquote><ol><li>将 spark-1.6.0-bin-hadoop2.6.tgz 放到/home/hadoop/目录下</li><li><p>解压文件到/usr/local 下，重命名文件夹并修改属主</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -xzvf spark<span class="number">-1.6</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>.tgz -C <span class="regexp">/usr/</span>local/</span><br><span class="line"></span><br><span class="line">sudo mv <span class="regexp">/usr/</span>local<span class="regexp">/spark-1.6.0-bin-hadoop2.6 /</span>usr<span class="regexp">/local/</span>spark</span><br><span class="line"></span><br><span class="line">sudo chown -R <span class="string">hadoop:</span>hadoop <span class="regexp">/usr/</span>local<span class="regexp">/spark/</span></span><br></pre></td></tr></table></figure></li><li><p>利用 spark 的 template 文件生成配置文件</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-env.<span class="keyword">sh</span>.template /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-env.<span class="keyword">sh</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cp /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/slaves.template /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/slaves</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cp /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-defaults.<span class="keyword">conf</span>.template /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-defaults.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure></li><li><p>修改 spark-env.sh，在文件末尾添加如下内容</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">HADOOP_HOME</span>=/usr/local/hadoop</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/lib/jvm/java-1.7.0-openjdk-amd64</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">HADOOP_CONF_DIR</span>=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_MASTER_IP</span>=master</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_LOCAL_DIRS</span>=/usr/local/spark</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_WORKER_MEMORY</span>=1000M</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_EXECUTOR_MEMORY</span>=1000M</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_DRIVER_MEMORY</span>=1000M</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_EXECUTOR_CORES</span>=3</span><br></pre></td></tr></table></figure></li></ol><p>这一步是为了配置spark的运行参数，hadoop_conf_dir的设置是为了让spark运行在yarn上。<br>几个 memory 命令分别用于设置 driver 和 executor 进程的内存，executor_cores 设置的是每个<br>executor 进程的 CPU cores 的数量，这些设置请依据自己的电脑实际可负载情况设置。</p><ol><li>修改 slaves 文件，在文件末尾添加其他节点 IP<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/usr/</span>local<span class="regexp">/spark/</span>conf<span class="regexp">/slaves</span></span><br></pre></td></tr></table></figure></li></ol><p>修改成<br></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">master</span></span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p></p><ol><li>修改 spark-defaults.conf，在文件末尾添加如下内容：<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /usr/<span class="keyword">local</span>/spark/<span class="keyword">conf</span>/spark-defaults.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure></li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark<span class="selector-class">.executor</span><span class="selector-class">.extraJavaOptions</span>  -XX:+PrintGCDetails -Dkey=value -Dnumbers=<span class="string">"one two three"</span></span><br><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.enabled</span>           true</span><br><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.dir</span>               hdfs:<span class="comment">//master:9000/historyserverforSpark</span></span><br><span class="line">spark<span class="selector-class">.yarn</span><span class="selector-class">.historyServer</span><span class="selector-class">.address</span> master:<span class="number">18080</span></span><br><span class="line">spark<span class="selector-class">.history</span><span class="selector-class">.fs</span><span class="selector-class">.logDirectory</span>    hdfs:<span class="comment">//master:9000/historyserverforSpark</span></span><br><span class="line">spark<span class="selector-class">.speculation</span>                true</span><br></pre></td></tr></table></figure><p>这一步是为保存 spark 的运行日志，并且是保存到 hdfs 上的文件夹里面，方便运维。</p><ol><li>将配置好的 spark 文件夹传到 slave1、slave2。</li></ol><ol><li>配置 hadoop:修改 yarn-site.xml 文件，添加新的属性。<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><p>这一步是为了开启日志整合功能，spark 在遇到问题时，利用命令<br>/usr/local/hadoop/bin/yarn logs -applicationId XXX<br>即可查看 applicationId 为 XXX 的实例的运行日志</p><p><strong>运行spark</strong></p><ol><li>开启hadoop集群</li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>sbin<span class="regexp">/start-all.sh</span></span><br></pre></td></tr></table></figure><ol><li>在 spark 中创建 historyserverforSpark 文件夹</li></ol><p>这个文件夹可以在网页上看见（hdfs）</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>bin<span class="regexp">/hdfs dfs -mkdir historyserverforSpark</span></span><br></pre></td></tr></table></figure><ol><li>运行 spark<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/spark/</span>sbin<span class="regexp">/start-all.sh</span></span><br></pre></td></tr></table></figure></li></ol><p>可以进入 spark 的 webui 查看是否成功启动：<code>192.1468.142.128:8080/</code><br>可以进入 spark 的 webui 查看节点是否成功启动：<code>192.1468.142.128:8080/cluster</code></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-7b362e81b839dc9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sparkstartall.png"></p><ol><li>运行 history-server，这样应用运行完的结果可以通过 webui 看到:<code>192.1468.142.128:18080</code></li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usr<span class="regexp">/local/</span>spark<span class="regexp">/sbin/</span>start-history-server.sh</span><br></pre></td></tr></table></figure><p>spark运行历史</p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-f9a73d0140a2a540.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="spark"></p><p><strong>运行spark实例</strong></p><ol><li>本地模式</li></ol><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/spark/bin/<span class="keyword">run</span><span class="bash">-example SparkPi &gt; SparkPiRes.txt</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">vi SparkPiRes.txt</span></span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/2560767-bc2628e8c05111dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Piresult.png"></p><ol><li>在yarn集群上运行</li></ol><p>运行可以有client模式和cluster模式<br>区别如下</p><blockquote><p>When run SparkSubmit —class [mainClass], SparkSubmit will call a childMainClass which is</p><ol><li><p>client mode, childMainClass = mainClass</p></li><li><p>standalone cluster mde, childMainClass = org.apache.spark.deploy.Client</p></li><li><p>yarn cluster mode, childMainClass = org.apache.spark.deploy.yarn.Client</p></li></ol><p>The childMainClass is a wrapper of mainClass. The childMainClass will be called in SparkSubmit, and if cluster mode, the childMainClass will talk to the the cluster and launch a process on one woker to run the mainClass.</p><p>ps. use “spark-submit -v” to print debug infos.</p><p>Yarn client: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master yarn JavaWordCount.jar<br>childMainclass: org.apache.spark.examples.JavaWordCount<br>Yarn cluster: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master yarn-cluster JavaWordCount.jar<br>childMainclass: org.apache.spark.deploy.yarn.Client</p><p>Standalone client: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master spark://aa01:7077 JavaWordCount.jar<br>childMainclass: org.apache.spark.examples.JavaWordCount<br>Stanalone cluster: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master spark://aa01:7077 —deploy-mode cluster JavaWordCount.jar<br>childMainclass: org.apache.spark.deploy.rest.RestSubmissionClient (if rest, else org.apache.spark.deploy.Client)</p></blockquote><p>Taking standalone spark as example, here is the client mode workflow. The mainclass run in the driver application which could be reside out of the cluster.</p><p>client 模式结构<br><img src="http://upload-images.jianshu.io/upload_images/2560767-60c0f8ea5ea9b781.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="clientmode.jpg"></p><p>On cluster mode showed as below, SparkSubmit will register driver in the cluster, and a driver process launched in one work running the main class.</p><p>cluster模式结构<br><img src="http://upload-images.jianshu.io/upload_images/2560767-e8388857afce784f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="clustermode.jpg"></p><p>yarn-client模式可以在运行中看见输出日志。如果是使用 yarn-cluster 模式，在运行过程中不会看到统计结果。需要到日志文件中查看。由于cluster模式，分配资源的driver不在本节点，无法把结果输出到命令行，但是通常cluster模式速度更快一些。因为driver是任务提交后自行分配的，资源和job分配更加合理。</p><p>键入命令运行wordcount<br></p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">/usr/local/spark/bin/spark-submit</span> <span class="params">--deploy-mode</span> cluster <span class="params">--master</span> yarn <span class="params">--driver-memory</span> 1G <span class="params">--class</span> org.apache.spark.examples.JavaWordCount <span class="params">--executor-memory</span> 1G <span class="params">--total-executor-cores</span> 2 <span class="string">/usr/local/spark/lib/spark-examples-1.6.3-hadoop2.6.0.jar</span> hdfs:<span class="string">//master</span><span class="function">:9000</span>/input</span><br></pre></td></tr></table></figure><p></p><p>出现下图表示程序无错误，运行成功</p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-6525d52795b26ae3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="spark成功.png"></p><p>本次测试使用的也是单词的统计<br>输入如下<br></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">a</span>.txt</span><br><span class="line">Hello world</span><br><span class="line">bye hadoop</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">b</span>.txt</span><br><span class="line">bye world</span><br><span class="line">hello hadoop hadoop</span><br></pre></td></tr></table></figure><p></p><p>键入命令查看输出测试的输出结果<br><code>/usr/local/hadoop/bin/yarn logs -applicationId XXX</code></p><p><img src="http://upload-images.jianshu.io/upload_images/2560767-a753be3edc411cd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sparkwordcount.png"></p><h3 id="五、Scala与Sbt的配置和测试"><a href="#五、Scala与Sbt的配置和测试" class="headerlink" title="五、Scala与Sbt的配置和测试"></a><strong>五、Scala与Sbt的配置和测试</strong></h3><p>这篇博文解释得非常清楚~<a href="https://blog.csdn.net/a532672728/article/details/72477591" title="blog">https://blog.csdn.net/a532672728/article/details/72477591</a></p><h3 id="六、Item-based-Collaboration-Filtering"><a href="#六、Item-based-Collaboration-Filtering" class="headerlink" title="六、Item-based Collaboration Filtering"></a><strong>六、Item-based Collaboration Filtering</strong></h3><p>见另一篇博文</p><h3 id="七、SlopeOne"><a href="#七、SlopeOne" class="headerlink" title="七、SlopeOne"></a><strong>七、SlopeOne</strong></h3><p>见另一篇博文</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Mapreduce-Hive-Spark平台搭建&quot;&gt;&lt;a href=&quot;#Mapreduce-Hive-Spark平台搭建&quot; class=&quot;headerlink&quot; title=&quot;Mapreduce,Hive,Spark平台搭建&quot;&gt;&lt;/a&gt;Mapreduce,Hive
      
    
    </summary>
    
      <category term="plantform" scheme="http://zhongfuze.github.io/categories/plantform/"/>
    
    
      <category term="spark,hive,hadoop,scala" scheme="http://zhongfuze.github.io/tags/spark-hive-hadoop-scala/"/>
    
  </entry>
  
  <entry>
    <title>Paper-Reading-Attention-based Transactional Context Embedding for Next-Item Recommendation</title>
    <link href="http://zhongfuze.github.io/2018/08/09/Paper-Reading/"/>
    <id>http://zhongfuze.github.io/2018/08/09/Paper-Reading/</id>
    <published>2018-08-09T02:46:45.000Z</published>
    <updated>2018-08-10T12:41:21.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Paper-Reading-——-Attention-based-Transactional-Context-Embedding-for-Next-Item-Recommendation"><a href="#Paper-Reading-——-Attention-based-Transactional-Context-Embedding-for-Next-Item-Recommendation" class="headerlink" title="Paper Reading —— Attention-based Transactional Context Embedding for Next-Item Recommendation"></a>Paper Reading —— Attention-based Transactional Context Embedding for Next-Item Recommendation</h2><p>基于注意力的事务上下文嵌入下一项推荐</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在电商交易环境中向user推荐下一个item,这样的应用非常实用但是具有挑战性。Transactional context 是指在交易记录中的observed items。<br>大多数现有的推荐系统，主要是考虑recently occurring items 而不是 all the ones observed in the current context，这些算法通常假设交易中的items之间存在严格的顺序，但是这并不总是起作用，a long transaction(一个比较长的交易范围)通常包含许多对下一个选择的item没有关联或者说是没有用的item信息，这往往会overwhelm一些真正相关的item的影响。<br></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">举个栗子~</span><br><span class="line">让我们举一个例子来说明上述问题。</span><br><span class="line">用户首先将三个项目&#123;milk，apple，<span class="keyword">orange&#125;放入购物车中，</span></span><br><span class="line"><span class="keyword">然后将&#123;bread&#125;添加到同一购物车中。</span></span><br><span class="line"><span class="keyword">随后，交易被确定为&#123;milk，apple，orange，bread&#125;。</span></span><br><span class="line"><span class="keyword">如果我们将前三个项目作为上下文而最后一个项目作为推荐的目标，</span></span><br><span class="line"><span class="keyword">现有方法可能会建议&#123;vegetables&#125;，如&#123;green </span>salad&#125;，</span><br><span class="line">因为最近的上下文项目（<span class="keyword">orange和apple）。</span></span><br><span class="line"><span class="keyword">但是，目标物品面包的选择可能取决于第一个项目（milk）。</span></span><br><span class="line"><span class="keyword">在这种情况下，推荐系统应该更多地关注milk而不是orange和apple，</span></span><br><span class="line"><span class="keyword">因为milk可能与下一个选择的bread更相关。</span></span><br><span class="line"><span class="keyword">此示例显示了下一项建议的重要性，这可能会被交易中的无关项误导。</span></span><br><span class="line"><span class="keyword">此外，真实世界的交易数据通常仅指示那些项目与项目之间的订单</span></span><br><span class="line"><span class="keyword">（例如，项目时间戳）共同出现在交易中。</span></span><br><span class="line"><span class="keyword">因此，推荐具有严格订单的交易项目可能是不可能和现实的。</span></span><br></pre></td></tr></table></figure><p></p><p>作者提出一个推荐算法，这个算法不仅考虑当前交易中所有的observed items，而且还要用不同的relevance(相关性)对它们进行加权，以建立一个attentive context(注意力上下文)，以高概率输出正确的下一个项目。模型——基于注意的事务嵌入模型（ATEM），用于上下文嵌入，以在不假定顺序的情况下对每个观察到的项目进行加权。对交易数据集的实证研究证明，ATEM在准确性和新颖性方面都显着优于最先进的方法。</p><h2 id="Main-Algorithm"><a href="#Main-Algorithm" class="headerlink" title="Main Algorithm"></a>Main Algorithm</h2><h3 id="问题描述与定义"><a href="#问题描述与定义" class="headerlink" title="问题描述与定义"></a>问题描述与定义</h3><p>推荐基于购物车序列( built onshoppingbasket-basedtransactiondata)</p><p>给定transactional dataset:</p><script type="math/tex;mode=display">T = \{t_{1},t_{2},...,t_{|T|}\}</script><p>给定每个transaction</p><script type="math/tex;mode=display">t =\{i_{1},i_{2},...,i_{|t|}\}</script><p>每个transaction是itemset的子集，且t里的并不是严格的交易顺序。</p><p>给定目标(target item) \(i<em>{s}\in t\)<br>除了item \(i</em>{s}\)<br>,<br>所有属于\(t\)<br>的items统称为context \(c\)</p><p>其中\(c=i_{s}\setminus t\)</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>推荐系统（RS）发挥着重要作用，特别是在商业领域。然而，大多数现有的RS理论面临各种问题，例如倾向于重复与用户可能已经选择的类似的项目。</p><p>在现实数据场景中，用户可能更喜欢新颖且与手头上现有的不同的项目。为了解决这个问题，需要在交易上下文中进行新的推荐，即在交易中已经选择的内容中挖掘。一方面，分析基于RS交易序列，通过分析交易间耦合关系，产生更合理和可靠的新交易建议，例如下一个购物车(basket)和下一项目(item)建议。这些与基于user profile和item profile构建的典型RS方法完全不同。</p><p>然而，当一个items集合被放入一个transaction时，仍然不清楚应该下一个项目应该推荐什么。这产生了通过分析事务内依赖性来推荐事务上下文下的下一项的需要。</p><p>(transactional context:用于推荐下一个项目的上下文是指对应的项目相关交易，例如，由多个所选项目组成的购物篮记录)</p><h2 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h2><p>了解transaction context中items之间的相关性和转换非常具有挑战性。在TBRS中，一个普遍的挑战是建立一个注意力(attention)的背景，以高概率输出真正的下一个选择。</p><p>一些现有方法旨在通过将transaction as the context来生成推荐。然而，大多数现有TBRS利用具有排序假设的部分上下文。</p><p><strong>顺序模式挖掘</strong>（2012）用于使用具有严格顺序假设的items之间的关联来预测下一项。但是，上下文中的项可能是任意的，这可能无法匹配任何已挖掘的模式。</p><p><strong>马尔可夫链（MC）</strong>（2012）是建模顺序数据的另一种方法。然而，MC只捕获从一个项目到下一个项目的转换，而不是从上下文序列中捕获，即，它只能捕获第一次转换。</p><p>最近，基于<strong>矩阵隐式因子分解（MF）</strong>的方法（2016）将转移概率的矩阵从然而，由于现实世界中的幂律分布数据，MF很容易受到稀疏性问题的困扰（2016）。</p><p>受Deep Learning的巨大成功的启发（2015），应用<strong>深度递归神经网络（RNN）</strong>来模拟顺序数据的事务，但由复杂结构引起的高计算成本阻止了其应用于大数据。</p><p>此外，<strong>MC，MF和RNN最初是为具有严格自然顺序的时间序列数据而设计的</strong>，因此它们不具有无序的交易。</p><p>(例如，或面包是否首先放入购物车中没有区别。另外，现有方法不能有效地加权上下文中的项目，即更多地关注那些相关项目。这种注意区分非常重要，特别是对于长期交易而言，这些交易往往包含许多与下一个选择无关的项目。)</p><p>最近，受心理认知方案的启发，注意机制在上下文学习相关方面显示出惊人的潜力。 通过搜索图像中与答案相关的区域，呈现用于图像问题回答的堆叠注意网络（SAN）。 另一个新模型在人类关注的指导下学习句子表征（Shaonan，Jiajun和Chengqing 2017）。 鉴于CV和NLP中的上下文学习注意机制的巨大成功，我们结合了一些想法并提出ATEM来模拟下一个项目推荐的注意上下文。</p><h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>本文通过提出一种基于注意力的交易嵌入模型（ATEM）来解决需求。 ATEM通过识别与下一个选择具有高度相关性的上下文项，在交易中的所有观察项目的嵌入（Embedding）上构建了一个关注的上下文（attention context）。构建了一个浅宽的广泛网络（wide-in-wide-out network）（Goth 2016），以减少时间和空间成本。具体而言，作者将注意机制（Shaonan，Jiajun和Chengqing 2017）纳入浅层网络，以在没有严格排序假设的情况下在事务中构建所有观察项目（observed items）的注意上下文(attention context)。由于注意机制，所提出的模型能够更多地关注更相关的items，而更少关注不太相关的items。因此，ATEM更有效，更强大，可以预测具有较少约束的事务中的下一个item。这项工作的主要贡献如下：</p><ul><li><p>基于注意力的模型学习一种注意力的上下文嵌入，强化了相关项目但忽略了与下一个选择无关的项目。 我们的方法不涉及对事务中项目的严格排序假设。</p></li><li><p>浅宽的宽广网络实现了ATEM，它对于大量项目的学习和预测更有效和高效。</p></li><li><p>实证研究表明</p><p>ATEM在准确性和新颖性方面明显优于两个真实数据集上的最新TBRS;</p><p>通过比较有无注意机制的方法，注意机制对TBRS产生显着差异。</p></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Paper-Reading-——-Attention-based-Transactional-Context-Embedding-for-Next-Item-Recommendation&quot;&gt;&lt;a href=&quot;#Paper-Reading-——-Attention-
      
    
    </summary>
    
      <category term="recommendation" scheme="http://zhongfuze.github.io/categories/recommendation/"/>
    
    
      <category term="embedding,recommendation" scheme="http://zhongfuze.github.io/tags/embedding-recommendation/"/>
    
  </entry>
  
  <entry>
    <title>我的第一篇文章</title>
    <link href="http://zhongfuze.github.io/2018/08/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/"/>
    <id>http://zhongfuze.github.io/2018/08/08/我的第一篇文章/</id>
    <published>2018-08-08T12:38:10.000Z</published>
    <updated>2018-08-10T07:41:02.290Z</updated>
    
    <content type="html"><![CDATA[<h3 id="hexo-搭建过程"><a href="#hexo-搭建过程" class="headerlink" title="hexo 搭建过程"></a>hexo 搭建过程</h3><h3 id="node-js"><a href="#node-js" class="headerlink" title="node.js"></a>node.js</h3><p>下载地址：<a href="http://nodejs.cn/download/" title="node.js下载地址">http://nodejs.cn/download/</a></p><h3 id="git环境"><a href="#git环境" class="headerlink" title="git环境"></a>git环境</h3><p>下载地址：<a href="https://gitforwindows.org/" title="git下载地址">https://gitforwindows.org/</a></p><h3 id="github"><a href="#github" class="headerlink" title="github"></a>github</h3><p><a href="https://github.com" title="github">https://github.com</a></p><h3 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h3><p>我的文件夹建立在D盘</p><p>打开Git Bash，输入：</p><p><code>cd d:</code></p><p><code>cd blog</code></p><p>接着，输入</p><p><code>npm install hexo</code></p><p>如果出现说npm没有安装的话，请移步编辑用户环境变量，将npm加入环境变量中，重启git bash即可。</p><p><a href="https://postimg.cc/image/d3p33i47f/"><img src="https://s25.postimg.cc/a9lxq221b/git.png" alt="git.png"></a></p><p>再次进入文件夹执行安装命令，如果出现<br><code>npm WARN deprecated titlecase@1.1.2: no longer maintained</code>可以不用理会</p><p><a href="https://postimg.cc/image/n103wlm3v/"><img src="https://s25.postimg.cc/y0lb87cj3/install_npm.png" alt="install_npm.png"></a></p><p>如果目录下出现文件夹就说明初始步骤基本成功了。</p><p><a href="https://postimg.cc/image/llyj7wvbf/"><img src="https://s25.postimg.cc/5ayfblitr/succeed.png" alt="succeed.png"></a></p><h3 id="hexo命令"><a href="#hexo命令" class="headerlink" title="hexo命令"></a>hexo命令</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo init  --初始化hexo环境,这时会在目录下自动生成hexo的文件</span><br><span class="line">npm install --安装npm依赖包</span><br><span class="line">hexo generate --生成静态页面</span><br><span class="line">hexo<span class="built_in"> server </span>--生成本地服务</span><br></pre></td></tr></table></figure><p>如果出现hexo command not found（笔者安装过程好坎坷啊！！！）</p><p><a href="https://postimg.cc/image/ydcped7nf/"><img src="https://s25.postimg.cc/tep6zu3un/hexo_not_find.png" alt="hexo_not_find.png"></a></p><p>解决方法：找到刚刚新建的blog文件夹，xxx/blog/hexo/bin<br>将这个路径添加到环境变量(xxx为自定义的路径,笔者的就是<code>D:/blog/hexo/bin</code>)</p><p>在blog路径下新建一个文件夹<code>hexo</code><br>执行hexo init需要在一个空的文件夹下执行，执行过程和结果如下图</p><p><a href="https://postimg.cc/image/eiqnsbkqz/"><img src="https://s25.postimg.cc/7fiscpfbj/success_install_hexp.png" alt="success_install_hexp.png"></a></p><p><a href="https://postimg.cc/image/n103wlm3v/"><img src="https://s25.postimg.cc/y0lb87cj3/install_npm.png" alt="install_npm.png"></a></p><p><a href="https://postimg.cc/image/utqrojx7v/"><img src="https://s25.postimg.cc/evi1yf2zz/generate_static_page.png" alt="generate_static_page.png"></a></p><p><a href="https://postimg.cc/image/851kp0xuj/"><img src="https://s25.postimg.cc/k6wyj6733/start_server.png" alt="start_server.png"></a></p><p>打开<code>http://localhost:4000/</code>看看是否启动成功，启动后会出现这个页面。</p><p><a href="https://postimg.cc/image/72re6jrwr/"><img src="https://s25.postimg.cc/4lfmza80f/hexo_page_in_web.png" alt="hexo_page_in_web.png"></a></p><h3 id="发布local到internet"><a href="#发布local到internet" class="headerlink" title="发布local到internet"></a>发布local到internet</h3><p>使用github创建远程仓库，新建一个跟自己账号名字一样的空仓库</p><p>接着打开本地的<code>hexo</code>文件夹中的_config.yml文件</p><p><a href="https://postimg.cc/image/k6wyj3erv/"><img src="https://s25.postimg.cc/9wujjuowf/config.png" alt="config.png"></a></p><p>打开文件编辑</p><p><a href="https://postimg.cc/image/nqiw90ziz/"><img src="https://s25.postimg.cc/qkm1mh1pb/open_config.png" alt="open_config.png"></a></p><p>文件末，将deploy修改如下：</p><p>(使用自己repo地址)</p><p><a href="https://postimg.cc/image/5ayfbl3e3/"><img src="https://s25.postimg.cc/7sa6iunan/my_git.png" alt="my_git.png"></a></p><p><a href="https://postimg.cc/image/rznmb37wb/"><img src="https://s25.postimg.cc/wyb4pmbpb/config_repo_setting1.png" alt="config_repo_setting1.png"></a></p><p><a href="https://postimg.cc/image/5ayfbiisr/"><img src="https://s25.postimg.cc/rmw84whwv/config_repo_setting2.png" alt="config_repo_setting2.png"></a></p><p>编辑完成后保存。</p><p>下面的步骤如果不想新建文章的话，到这就结束了<br>保存后再Git Bash 中执行</p><pre><code>``npm install hexo-deployer-git --save``最后执行``hexo deploy``这一步需要保证github上拥有本机的公钥！！！</code></pre><blockquote><p>备注：添加公钥方法</p><p>本地创建SSH Keys<br><code>ssh-keygen -t rsa -C &quot;xxx@xxx.com&quot;</code></p><p>邮箱即为 github 注册邮箱,之后会要求确认路径和输入密码，一路回车。</p><p>成功的话会在 ~/下生成 .ssh文件夹，进去，打开 id_rsa.pub，复制里面的key即可。</p><p><code>cat ~/.ssh/id_rsa.pub</code></p><p>复制内容至github-Setting-GPG and SSH Keys-Add</p></blockquote><h3 id="新建文章"><a href="#新建文章" class="headerlink" title="新建文章"></a>新建文章</h3><p>在Git Bash输入<br><code>hexo new World</code></p><blockquote><p>注：World 是标题</p></blockquote><p><a href="https://postimg.cc/image/utqrol7ij/"><img src="https://s25.postimg.cc/xb2ivurf3/new_page.png" alt="new_page.png"></a></p><p>在<code>xxx/blog/hexo/source/_post</code>文件夹里会有<br>World.md</p><p>内容如下：</p><p><a href="https://postimg.cc/image/ra4tytf3f/"><img src="https://s25.postimg.cc/wljqjj167/Wordmd.png" alt="Wordmd.png"></a></p><p>在之后使用md格式编辑正文并保存。</p><h3 id="hexo-部署"><a href="#hexo-部署" class="headerlink" title="hexo 部署"></a>hexo 部署</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//生成静态文件</span></span><br><span class="line">hexo <span class="keyword">generate</span></span><br></pre></td></tr></table></figure><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//安装 hexo-deployer-git</span></span><br><span class="line">npm install hexo-deployer-git --<span class="keyword">save</span></span><br></pre></td></tr></table></figure><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">//</span>部署到github</span><br><span class="line">hexo <span class="keyword">deploy</span></span><br></pre></td></tr></table></figure><p><a href="https://postimg.cc/image/9wujjx1rv/"><img src="https://s25.postimg.cc/y0lb87k8v/down.png" alt="down.png"></a></p><p>打开<a href="http://zhongfuze.github.io">http://zhongfuze.github.io</a></p><p>完毕</p><h3 id="Theme"><a href="#Theme" class="headerlink" title="Theme"></a>Theme</h3><p><a href="https://hexo.io/themes/" title="hexo">https://hexo.io/themes/</a></p><h3 id="mathjax"><a href="#mathjax" class="headerlink" title="mathjax"></a>mathjax</h3><p><a href="https://blog.csdn.net/u014630987/article/details/78670258" title="mathjax">https://blog.csdn.net/u014630987/article/details/78670258</a></p><h3 id="markdown"><a href="#markdown" class="headerlink" title="markdown"></a>markdown</h3><p>markdownpad ： <a href="http://markdownpad.com/download.html" title="markdown">http://markdownpad.com/download.html</a></p><p>awesomium ： awesomium_v1.6.6_sdk_win.exe</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;hexo-搭建过程&quot;&gt;&lt;a href=&quot;#hexo-搭建过程&quot; class=&quot;headerlink&quot; title=&quot;hexo 搭建过程&quot;&gt;&lt;/a&gt;hexo 搭建过程&lt;/h3&gt;&lt;h3 id=&quot;node-js&quot;&gt;&lt;a href=&quot;#node-js&quot; class=&quot;he
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://zhongfuze.github.io/2018/08/08/hello-world/"/>
    <id>http://zhongfuze.github.io/2018/08/08/hello-world/</id>
    <published>2018-08-08T09:11:56.371Z</published>
    <updated>2018-08-08T09:11:56.373Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for
      
    
    </summary>
    
    
  </entry>
  
</feed>
