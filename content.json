{"meta":{"title":"F泽泽馥泽泽杂记F","subtitle":"Hello World","description":"嘤嘤嘤","author":"ZhongFuze","url":"http://zhongfuze.github.io"},"pages":[],"posts":[{"title":"LANE","slug":"LANE","date":"2018-08-31T07:52:50.000Z","updated":"2018-08-31T08:00:07.287Z","comments":true,"path":"2018/08/31/LANE/","link":"","permalink":"http://zhongfuze.github.io/2018/08/31/LANE/","excerpt":"","text":"带标签数据的sku嵌入方法方法名：Label Informed Attributed Network Embedding简称：LANEsku嵌入向量中应包括：user对sku的行为，sku属性，sku标签算法基本流程从用户对sku的pv序列构造网络清洗出sku属性输入模型计算嵌入 LANE(network,attribute,(label),dim)sku嵌入向量评估输入seq2seq训练预测LANE 细节网络的构造从hive表里得到用户对sku的pv序列将相邻的sku关系，设置为网络中node之间有一条有向边算法伪代码Algorithm :Label Informed Attributed Network EmbeddingInput: \\(d\\)（嵌入维度）Input: \\(max-iter\\)（迭代次数）Input: \\(G\\)（带权邻接矩阵）Input: \\(A\\)（属性矩阵）Input: \\(\\alpha {1}$,$\\alpha{2}\\)（权重参数）Output:H（sku嵌入矩阵）设sku数量(即构造图中的节点数量)为\\(n\\)，sku属性的维度为\\(m\\), sku标签的维度为\\(k\\)，sku嵌入向量维度为\\(d\\)G \\in R^{n*n}, A \\in R^{n*m}, Y \\in R^{n*k}S^{(G)},S^{(A)} \\in R^{(n*n)}L^{(G)}, L^{(A)}, L^{(Y)} \\in R^{n*n}U^{(G)}, U^{(A)}, U^{(Y)},H \\in R^{n*d}1 : Construct the affinity matrices \\(S^{(G)}\\) and \\(S^{(A)}\\)2 : Compute Laplacian matrices \\(L^{(G)}\\) , \\(L^{(A)}\\) and \\(L^{(Y)}\\)3 : Initialize \\(t = 1$, \\(U^{(A)}=0, U^{(Y)} =0,H=0\\)4 : repeat5 : &nbsp;&nbsp;&nbsp;&nbsp; Update \\(U^{(G)}\\)(L^{(G)} + \\alpha_{1} U^{(A)} U^{(A)^{T}} + \\alpha_{2} U^{(Y)} U^{(Y)^{T}} + HH^{T})U^{(G)} = \\lambda_{1}U^{(G)}6 : &nbsp;&nbsp;&nbsp;&nbsp; Update \\(U^{(A)}\\)(\\alpha_{1}L^{(A)} + \\alpha_{1} U^{(G)} U^{(G)^{T}} + HH^{T})U^{(A)} = \\lambda_{2}U^{(A)}7 : &nbsp;&nbsp;&nbsp;&nbsp; Update \\(U^{(A)}\\)(\\alpha_{2}L^{(YY)} + \\alpha_{2} U^{(G)} U^{(G)^{T}} + HH^{T})U^{(Y)} = \\lambda_{3}U^{(Y)}8 : &nbsp;&nbsp;&nbsp;&nbsp; Update \\(H\\)(U^{(G)} U^{(G)^{T}} + U^{(A)} U^{(A)^{T}} + U^{(Y)} U^{(Y)^{T}})H = \\lambda_{4}H9 : \\(t = t +1\\)10 : until max-iter11 : return H本周工作spark版本http://git.jd.com/ads_ad_track/lanke_spark_new/tree/graph_embed流程：构造sku_pv序列 —&gt; 构造网络 —&gt;构造属性 —&gt;提取标签(暂时用三级类目代替) —&gt; 输入LANE学习sku嵌入表示其中在LANE模型中，求解拉普拉斯矩阵的时候算法卡住，卡住的地方为spark.mllib.BlockMatrix 中的一个reduce操作没有解决bug，所以使用tensorflow再写了一个版本python-tensorflow版本http://git.jd.com/ads_ad_track/lanke_tf/tree/lane流程：使用spark构造好网络(稀疏表示),属性,标签等数据， 进而将paquet文件转为tfrecords格式再输入LANE模型中遇到的问题：在9n集群上跑完结果之后，保存嵌入结果，上传至hdfs。但是将结果写成单个文件太大，且io太慢老集市马上下线了，之前一直在老集市上使用9n集群，8-30日将数据转移至新集市，准备测试并保存模型，我只有ads_ordertrack堡垒机200的公共账号，在堡垒机200上使用自己的ERP登陆9n-mta资源组，导致钟雨41正在运行的任务失败，所以还没有在新集市上进行测试，等待解决方法中下周安排保存的嵌入结果，分成小文件存储，再上传至hdfs使用别的组训练好的商品的特征，我找到了用bgn的embedding的cf： https://cf.jd.com/pages/viewpage.action?pageId=129775930该嵌入向量共2400维，现在seq2seq是pv2order，还可以使用order2order，cart2order等模式学习keras，并结合一个案例，比如咱们的某个应用用keras重写，做分享","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://zhongfuze.github.io/categories/Algorithm/"}],"tags":[{"name":"GraphEmbedding","slug":"GraphEmbedding","permalink":"http://zhongfuze.github.io/tags/GraphEmbedding/"}]},{"title":"SparkRDD-备忘","slug":"SparkRDD","date":"2018-08-21T08:20:27.000Z","updated":"2018-08-21T09:11:13.908Z","comments":true,"path":"2018/08/21/SparkRDD/","link":"","permalink":"http://zhongfuze.github.io/2018/08/21/SparkRDD/","excerpt":"","text":"RDD是spark_1.x 特有的数据模型RDD：Resilient Distributed Dataset,弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。这么多好处，必然也有坏处，RDD是不可变的，也就是说在产生RDD之后，将不能改变RDD中的值，不像数组那样可以通过索引的方式随意更改内容。如果非要更改，只能产生一个新的RDD，然后覆盖。不可更改的特性易于分布式处理，但是如果使用RDD进行训练的话，生写会非常不便，迭代更新RDD不现实。所谓自动容错，就是RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。DataSet、DataFrame（Dataset[Row]）是spark_2.x特有的数据类型，RDD和Dataset之间可以互相转换本文归纳RDD的算子作为备忘textFile,parallelize构造RDD从本地文件，或者从HDFS读取,textFile(pathString,num),num的精简意思是将文件分成num份存在分布式文件系统上,num也可以缺省，使用默认值12val rdd:RDD[String] = sc.textFile(\"file:///usr/local/data.txt\", 3)val rdd:RDD[String] = sc.textFile(\"hdfs://ns3/user/ads_ordertrack/data/data.txt\", 3)这样读入的rdd需要更多的处理12345[data.txt]1,2,31,3,41,3,5...那便还需要自己分割，转化成数据123456val rdd:RDD[String] = sc.textFile(\"hdfs://ns3/user/ads_ordertrack/data/data.txt\", 3)val SpliteRDD = rdd.map(line =&gt; &#123; val fields = line.split (\",\") (fields(0).toLong, fields(1).toInt, fields(2).toDouble)&#125;)从数组(Array,ArratBuffer)，从列表(List)，或者从元组(Tuple)生成RDD1val a = sc.parallelize(Array(\"a\",\"b\",\"c\",\"d\"),3)saveAsTextFile保存RDD保存RDD至HDFS或者本地,saveAsTextFile(outputPath,num)num也可以缺省,num精简意思就是保存数据的文件个数。若num=3那便可在保存的路径文件夹里，看见形如part-00xx这样的文件，直接vi是看不到内容的使用cat part-00* &gt; data_saved.txt查看内容12val outputPath = \"hdfs://ns3/user/ads_ordertrack/data/\"a.saveAsTextFile(outputPath, 3)","categories":[{"name":"spark","slug":"spark","permalink":"http://zhongfuze.github.io/categories/spark/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://zhongfuze.github.io/tags/scala/"}]},{"title":"Paper-Reading-Billion-scale Commodity Embedding for E-commerce Recommendation","slug":"Paper-Reading2","date":"2018-08-15T02:46:45.000Z","updated":"2018-08-15T06:48:19.603Z","comments":true,"path":"2018/08/15/Paper-Reading2/","link":"","permalink":"http://zhongfuze.github.io/2018/08/15/Paper-Reading2/","excerpt":"","text":"Paper Reading —— Attention-based Transactional Context Embedding for Next-Item Recommendation用于电子商务推荐的十亿规模商品嵌入AbstractMain AlgorithmMotivationRelatedContribution1）不同项目的权重分布是不同的，这与我们的假设一致，即不同的边信息对最终表示的贡献不同。2）在所有项目中，表示项目本身的嵌入的“项目”的权重始终大于所有其他辅助信息的权重。 它证实了一种直觉，即项目本身的嵌入仍然是用户行为的主要来源，而辅助信息则提供了用于推断用户行为的额外提示。3）除“项目”外，“商店”的权重始终大于对方信息的权重。它与淘宝的用户行为一致，即用户倾向于在同一商店购买物品，方便，价格较低。Summary","categories":[{"name":"recommendation","slug":"recommendation","permalink":"http://zhongfuze.github.io/categories/recommendation/"}],"tags":[{"name":"embedding,recommendation","slug":"embedding-recommendation","permalink":"http://zhongfuze.github.io/tags/embedding-recommendation/"}]},{"title":"Plantform","slug":"Plantform","date":"2018-08-10T07:43:31.000Z","updated":"2018-08-15T06:51:51.678Z","comments":true,"path":"2018/08/10/Plantform/","link":"","permalink":"http://zhongfuze.github.io/2018/08/10/Plantform/","excerpt":"","text":"Mapreduce,Hive,Spark平台搭建说明平台搭建成功后，使用Scala语言进行算法设计和应用的开发。文末有使用scala设计的Item-Based Collaboration Filtering和SlopeOne 两个协同过滤算法一、搭建准备VMWare WorkstationUbuntu 14.04 Server.isoXshell——远程连接主机终端Server 版安装配置新建三台虚拟机，安装时选择OpenSHH Server预安装环境一台作为master另两台作为slave，命名为slave1和slave2master的配置比slave要稍微高一些IP 设置VMnet8是VM配置的虚拟网卡设置VMnet8，选择「NAT 模式」设置子网IP （192.168.142.0）和子网掩码（255.255.255.0）Ubuntu 系统的登录和 IP 的固化输入 ifconfig 回车查询当前的 ip 地址，准备将其长期绑定到当前虚拟机,为了方便后续实验。master的ip：192.168.142.128slave1的ip：192.168.142.130slave2的ip：192.168.142.132此时的 IP 地址是由 DHCP 服务器动态分配的，为了让这个 IP 地址能一直与这台虚拟机绑定，我们需要改变系统启动时获取 IP 的方式，从 DHCP 切换到静态 IP地址，为此需要编辑 Linux 的网卡配置文件（/etc/network/interfaces），输入命令sudo vi /etc/network/interfaces回车，可以看到 eth0 的 IP 获取方式是 dhcp：此时需要修改 ip 的获取方式从 DHCP 到 static，设置静态的 ip 地址、子网掩码和默认网关。把这个文件更改为：12345678910111213#This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto eth0iface eth0 inet staticaddress 192.168.142.128gateway 192.168.142.2netmask 255.255.255.0为了进一步固化 DNS 服务器 IP，输入sudo vi /etc/resolvconf/resolv.conf.d/bas将里面的内容替换成nameserver 192.168.142.2，vi 保存系统的登录和 IP 的固化重启后完成使用Xshell连接三个虚拟机终端下载lrzsz工具用于虚拟机与宿主机之间文件的上传和下载sudo apt-get updatesudo apt-get install lrzszrz命令，从宿主机传文件至虚拟机sz命令，从虚拟机获取文件至宿主机二、Hadoop 及相关环境的安装开源分布式平台 Hadoop 可以聚合多个计算机形成集群，在各个节点上安装配置完Hadoop后可以直接提交分布式代码到集群计算。本次实验可以在个人电脑上用VMware完成，或使用天河二号上的 OpenStack平台创建内存为2G的虚拟机资源完成。安装包准备Hadoop环境——hadoop-2.6.0.tar.gzJava环境——jdk-8u60-linux-x64.tar.gz虚拟机ip配置测试123sudo vi /etc/hosts #编辑 /etc/hosts 文件，插入角色与 IP 映射ping master -c 4 #尝试用角色名 ping 其它主机，一次 4 个包hosts文件修改为：12345678910127.0.0.1 localhost192.168.142.128 master192.168.142.130 slave1192.168.142.132 slave2# The following lines are desirable for IPv6 capable hosts::1 localhost ip6-localhost ip6-loopbackff02::1 ip6-allnodesff02::2 ip6-allrouters三个虚拟机能够使用主机名（不是ip）ping通即配置正确配置 SSH 无密码登录保障了 Hadoop 可以通过角色名在局域网里找到各个节点，为了让 Hadoop 可以进一步读取、操作各个节点，需要赋予其登录的权限，意即让 Hadoop 拥有各个节点的普通用户账号，从而在需要操作各个节点时直接用对应的账号登录获取操作权限。SSH 协议可以为节点上的账户创建唯一的公私钥，然后利用这些公私钥实现无密码登录，从而让 Hadoop 直接绕开传统的账号密码登录过程，直接用公私钥访问节点。生成各个节点的 SSH 公私钥：123cd ~/.ssh # 如果没有该目录，先执行一次 ssh localhostrm ./id_rsa* # 删除之前生成的公匙（如果有）ssh-keygen -t rsa # 一直按回车就可以为了让每个节点都拥有其它节点的公钥，要先把所有公钥放进一个文件里在 master 上，将 master 的公钥复制到 authorized_keys 文件里：1cat ./id_rsa.pub &gt;&gt; ./authorized_keys # cat 命令用于提取内容，&gt;&gt;输出重定向将 slave1、slave2 的公钥文件发送给 master，此时的传送要输入密码：1scp ~/.ssh/id_rsa.pub hadoop@master:/home/hadoop/Master 将接收到的 slave1 的公钥文件里的内容提取追加到 authorized_keys 文件里：1cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys将 slave2 的公钥内容也放进 authorized_keys 文件，然后将authorized_keys 文件分别发送到两个 slave 的~/.ssh/下：1scp ~/.ssh/authorized_keys hadoop@slave1:/home/hadoop/.ssh/搭建成功表现：每个节点尝试使用 ssh &lt;角色名&gt;的命令直接登录其它节点，直到每个节点都可以成功免密码登录其它节点，则免密码登录配置成功！ 如在 master 上输入：ssh slave1即可直接登陆slave1的虚拟机，不需要再输入密码登陆，便于后续实验操作。配置 Java环境和Hadoop环境安装包准备：jdk-8u60-linux-x64.tarhadoop-2.6.0.tar.gz安装 JDK将上传的 JDK 压缩包（jdk-8u60-linux-x64.tar）放到根目录/home/hadoop/，解压并放到指定的文件夹：12sudo mkdir -p /usr/local/jvmtar -zxvf jdk-8u60-linux-x64.tar.gz -C /usr/local/jvm安装 Hadoop在各个节点上将 hadoop 解压到/usr/local/目录下，改变其所属用户和所属组（让hadoop 软件用 hadoop 账号登录时对 hadoop 文件夹拥有最高权限）：123tar -zxvf hadoop-2.6.0.tar.gz -C /usr/local/sudo mv /usr/loca/hadoop-2.6.0 /usr/local/hadoop #mv 实现重命名sudo chown -R hadoop:hadoop /usr/local/hadoop将当前的 PATH 环境变量提取保存到 P.sh12345678910环境变量如下：export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/jvm/jdk1.8.0_60/bin:/usr/local/jvm/jdk1.8.0_60/bin:/usr/local/hadoop/bin:/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/jvm/jdk1.8.0_60/bin:/usr/local/hive/binexport HADOOP_HOME=/usr/local/hadoopexport HIVE_HOME=/usr/local/hiveexport JAVA_HOME=/usr/local/jvm/jdk1.8.0_60export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib:$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATHexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin:$HADOOP_HOME/bin:$HADOOP/sbin:$PATH:$&#123;HIVE_HOME&#125;/bin之后每次重启系统都需要重定向环境变量12echo $PATH &gt;&gt; ~/setenv.shvi ~/P.sh使环境变量生效：1source ~/setenv.sh查看java版本信息，如果出现版本信息则环境配置成功12java -versionjavac -version重点：hadoop节点配置修改 slaves 文件，让 hadoop 知道自己可以聚合的节点名（保证与 hosts 里的角色名一致）1vi /usr/local/hadoop/etc/hadoop/slaves123masterslave1slave2修改 core-site.xml 文件如下：配置端口和路径1vi /usr/local/hadoop/etc/hadoop/core-site.xml1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;修改 hdfs-site.xml 文件如下（启用所有节点作为 DataNode，包括master故 replication_value=3）：当节点增多时，需要更改配置文件，如主机名、IP地址、节点信息等配置都要重新修改1vi /usr/local/hadoop/etc/hadoop/hdfs-site.xml1234567891011121314151617&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hdfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;修改 mapred-site.xml 文件如下：1vi /usr/local/hadoop/etc/hadoop/mapred-site.xmlyarn为集群的表示1234567&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;修改 yarn-site.xml 文件如下（启用 yarn 资源管理器），为大数据计算分配计算、存储资源等1vi /usr/local/hadoop/etc/hadoop/yarn-site.xml123456789101112&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;修改 hadoop-env.sh 文件，将 25 行 JAVA_HOME 的值换成 jdk 所在的路径：1vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh当前jdk路径为：JAVA_HOME=/usr/local/jvm/jdk1.8.0_60重点：hadoop启动及验证对 hadoop 进行 NameNode 的格式化：/usr/local/hadoop/bin/hdfs namenode -format启动 hdfs 和 yarn，并在各个节点上输入 jps 查看启动的服务：只需在master上启动1234567/usr/local/hadoop/sbin/start-dfs.sh/usr/local/hadoop/sbin/start-yarn.sh或者/usr/local/hadoop/sbin/start-all.shjps # 每个节点都查看一次尝试在 hdfs 上创建输入文件夹input，并把 测试的txt文本文件放进去：123/usr/loca/hadoop/bin/hdfs dfs -mkdir /input/usr/local/hadoop/bin/hdfs dfs -put /usr/local/hadoop/a.txt /input/usr/local/hadoop/bin/hdfs dfs -put /usr/local/hadoop/b.txt /input1234567a.txtHello worldbye hadoopb.txtbye worldhello hadoop hadoop可以在外部浏览器输入 master 的 IP 地址和 50070 端口查看 hdfs 上的文件输入：http://192.168.142.128:50070尝试用 写一个简单的 WordCount.java代码，统计上面文本文件中每个单词出现的频数：hadoop的工作是把大任务切分成小jobs，再通过资源管理器分配存储和计算等资源给相应hadoop集群内的节点1234/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /input /output/usr/local/hadoop/bin/hdfs dfs -cat /output/* #查看输出结果hadoop 搭建完成三、Hive 数据仓库安装部署及测试安装包准备apache-hive-1.2.0-bin.tar.gzmysql-connector-java-5.1.44-bin.jartestdata.zipHive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来 进行数据提取转化加载（ETL） ），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。另外，它定义了简单的类 SQL 查询语言，称为 HQL，允许熟悉 SQL 的用户查询数据。如果是实施的交通数据，那么搭建动态的数据仓库是很有必要的。原理图Hive的工作原理简单来说就是一个查询引擎Hive的架构图：Hive的工作原理如下：接收到一个sql,后面做的事情包括：1.词法分析/语法分析使用antlr将SQL语句解析成抽象语法树-AST2.语义分析从Megastore获取模式信息，验证SQL语句中队表名,列名，以及数据类型的检查和隐式转换，以及Hive提供的函数和用户自定义的函数（UDF/UAF）3.逻辑计划生产生成逻辑计划-算子树4.逻辑计划优化对算子树进行优化，包括列剪枝，分区剪枝，谓词下推等5.物理计划生成将逻辑计划生产包含由MapReduce任务组成的DAG的物理计划6.物理计划执行将DAG发送到Hadoop集群进行执行7.将查询结果返回流程如下图:hive和mapreduce共同运作用户接口CLI：就是 Shell 命令行.JDBC：这个是 Hive 的 java 接口，与通常的数据库类似.WebGUI：网页界面。驱动组件（Driver）Hive 的编译、解析、优化转化为 MapReduce 任务提交给 Hadoop 进行分派和执行相应的任务。元数据组件（Metatore）存储着 hive 的元数据信息，包括表名、列、分区和属性等。默认数据库为Derby，为了更健壮，一般使用 Mysql 来代替。另外，MetaStore 分为服务端和客户端，服务端提供给客户端对 RDBMS 的访问服务。安装 Mysql1sudo apt-get install mysql-server安装完成后设置数据库root用户的密码，123456配置 Mysql1mysql –uroot –p 的 使用初始化时设置的 root 密码登录新增 hive 用户，并给于权限：123create user 'hive' identified by 'hive';grant all privileges on *.* to 'hive' with grant option;flush privileges;Hive 安装与配置安装 hive将 apache-hive-1.2.0-bin.tar.gz 解压在/usr/local 目录下1sudo tar –zxvf apache-hive-1.2.0-bin.tar.gz –C /usr/local重命名文件夹为 hive 文件夹，并将其权限修改成 hadoop12mv /usr/local/ apache-hive-1.2.0-bin /usr/local/hivesudo chown -R hadoop:hadoop /usr/local/hive把 mysql 的 jdbc 的驱动 mysql-connector-java-5.1.44-bin.jar 拷贝到\\usr\\local\\hive\\lib 目录下1cp mysql-connector-java-5.1.44-bin.jar /usr/local/hive/lib配置环境变量修改之前的P.sh12345678910环境变量如下：export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/jvm/jdk1.8.0_60/bin:/usr/local/jvm/jdk1.8.0_60/bin:/usr/local/hadoop/bin:/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/jvm/jdk1.8.0_60/bin:/usr/local/hive/binexport HADOOP_HOME=/usr/local/hadoopexport HIVE_HOME=/usr/local/hiveexport JAVA_HOME=/usr/local/jvm/jdk1.8.0_60export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib:$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATHexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin:$HADOOP_HOME/bin:$HADOOP/sbin:$PATH:$&#123;HIVE_HOME&#125;/bin修改完成后source P.shHive 的配置hive 有关于 metastore 具有三种配置，分别为内嵌模式、本地元存储以及远程在 元存储.目前在 namenode 上配置服务端，datanode 上配置为客户端，在 datanode行 进行 hql 时通过远程模式访问在 namenode 的元数据。（实际场景 metastore 的数据库可以在任何节点，以减轻 namenode 的压力）在 /usr/local/hive/conf 目录下创建 hive-site.xml 文件：1sudo vi /usr/local/hive/conf/hive-site.xml在 server 端配置 hive-stie.xml,修改后保存ConnectionURL 属性用于设置 mysql 服务所在地址与端口，这里 mysql-server 在 在本地， hive.metastore.warehouse.dir 是在 HDFS 上的文件路径，hive.metastore.local 的 的为 值为 true 表示对 metastore 的访问为本地模式。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/db_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; ///////////////////////////////////////////////////////////////// &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;192.168.142.128&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.schedulaer.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;在 client 端配置 hive-stie.xml,修改后保存hive.metastore.uris 项指向提供数据库访问的 metastore 服务端，值须为 IP 地 地了 址。由于设置了 uris 的内容，因而对于 metastore 的访问默认为远程模式。123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://192.168.142.128:9083&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;运行 Hive检查 jline 版本，hive 与 hadoop 的 jline 版本不对应可能导致运行错误，先删除之前的旧版本，然后将 hive 上 jline 的 jar 包拷贝至 hadoop 的对应目录下：1cp /usr/local/hive/lib/jline-2.12.jar /usr/local/hadoop/share/hadoop/yarn/lib更新 yarn-site.xml 配置重要：为了使得 mapreduce 程序可以在各个节点提交，对各个节点的 hive-site.xml 配置文件 追加下 以下 property其中 yarn.resourcemanager.hostname 配置的是运行 ResourceManager 进程所在的主机（master 节点）IP 地址。12345678910111213141516171819&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;192.168.142.128&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.schedulaer.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt;只需在master上进行：启动hadoop：/usr/local/hadoop/sbin/start-all.sh初始化 Schema1/usr/local/hive/bin/./schematool –dbType mysql –initSchema服务端启动 metastore 服务1hive --service metastore接着在slave1和slave2启动hive的客户端1hive测试hive数据仓库：导入数据可以是使用Tab间隔的原数据形式,从本地导入。也可以从 HDFS 中导入。与本地的类似，只要把本地载入的命令中’local’去掉，输入路径即为HDFS 上的路径。12345list1.txt10001 1003 c2sld54sfkid paul10002 1014 d5dsd32sdkif rose10005 1008 e0dlp92mklpi harry10203 1099 p3skd29llsie bob12345list2.txt20004 2991 e2ifp12adlpi alice20132 1014 l8doo32haodp jerry50232 3022 d3sod41dsooo smith40001 1023 s3dfdlksdfj3 rose登入 hive 客户端，创建 user_info 表12345&gt;hive&gt;create table user_info(uid int, did int, pwd string, uname string)&gt;row format delimited&gt;fields terminated by ‘\\t’&gt;lines terminated by ‘\\n’;把数据加载到表:1load data local inpath ‘list*.txt’ into table user_info;将查询结果插入到表说明：将所有名为 rose 的记录插入到 account_rose 表中先创建 account_rose 表123456create table user_info(uid int,did int,pwd string,uname string)row format delimitedfields terminated by '\\t'lines terminated by '\\n'select * from user_info;Hive数据仓库建立成功动态数据仓库可以使用jsp进行动态的添加,结合Android、html或者其他可视化工具进行操作四、spark的配置和测试Spark 是专为大规模数据处理而设计的快速通用的计算引擎，有与 hadoop 相似的开源集群计算环境，但是能在某些工作负载方面表现得更加优越，换句话说，spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。spark 有三种模式，分别是 standalone、spark on yarn，spark on mesos，这里我们选择 spark onyarn 的模式，即在我们之前部署的 hadoop 集群上部署 spark。我的电脑是 1G 运行内存，所以设置的是 1000M，executor cores 是 2（经过测试 driver memory 和 worker。memory 如果小于 512M，executor cores 大于 2 的话用 spark 提交任务会很容易就崩死，提示的是 SparkContext 相关错误）。将 spark-1.6.0-bin-hadoop2.6.tgz 放到/home/hadoop/目录下解压文件到/usr/local 下，重命名文件夹并修改属主12345sudo tar -xzvf spark-1.6.0-bin-hadoop2.6.tgz -C /usr/local/sudo mv /usr/local/spark-1.6.0-bin-hadoop2.6 /usr/local/sparksudo chown -R hadoop:hadoop /usr/local/spark/利用 spark 的 template 文件生成配置文件1234567cp /usr/local/spark/conf/spark-env.sh.template /usr/local/spark/conf/spark-env.shcp /usr/local/spark/conf/slaves.template /usr/local/spark/conf/slavescp /usr/local/spark/conf/spark-defaults.conf.template /usr/local/spark/conf/spark-defaults.conf修改 spark-env.sh，在文件末尾添加如下内容123456789export HADOOP_HOME=/usr/local/hadoopexport JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport SPARK_MASTER_IP=masterexport SPARK_LOCAL_DIRS=/usr/local/sparkexport SPARK_WORKER_MEMORY=1000Mexport SPARK_EXECUTOR_MEMORY=1000Mexport SPARK_DRIVER_MEMORY=1000Mexport SPARK_EXECUTOR_CORES=3这一步是为了配置spark的运行参数，hadoop_conf_dir的设置是为了让spark运行在yarn上。几个 memory 命令分别用于设置 driver 和 executor 进程的内存，executor_cores 设置的是每个executor 进程的 CPU cores 的数量，这些设置请依据自己的电脑实际可负载情况设置。修改 slaves 文件，在文件末尾添加其他节点 IP1vi /usr/local/spark/conf/slaves修改成123masterslave1slave2修改 spark-defaults.conf，在文件末尾添加如下内容：1vi /usr/local/spark/conf/spark-defaults.conf123456spark.executor.extraJavaOptions -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"spark.eventLog.enabled truespark.eventLog.dir hdfs://master:9000/historyserverforSparkspark.yarn.historyServer.address master:18080spark.history.fs.logDirectory hdfs://master:9000/historyserverforSparkspark.speculation true这一步是为保存 spark 的运行日志，并且是保存到 hdfs 上的文件夹里面，方便运维。将配置好的 spark 文件夹传到 slave1、slave2。配置 hadoop:修改 yarn-site.xml 文件，添加新的属性。123456789101112&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;这一步是为了开启日志整合功能，spark 在遇到问题时，利用命令/usr/local/hadoop/bin/yarn logs -applicationId XXX即可查看 applicationId 为 XXX 的实例的运行日志运行spark开启hadoop集群1/usr/local/hadoop/sbin/start-all.sh在 spark 中创建 historyserverforSpark 文件夹这个文件夹可以在网页上看见（hdfs）1/usr/local/hadoop/bin/hdfs dfs -mkdir historyserverforSpark运行 spark1/usr/local/spark/sbin/start-all.sh可以进入 spark 的 webui 查看是否成功启动：192.1468.142.128:8080/可以进入 spark 的 webui 查看节点是否成功启动：192.1468.142.128:8080/cluster运行 history-server，这样应用运行完的结果可以通过 webui 看到:192.1468.142.128:180801usr/local/spark/sbin/start-history-server.shspark运行历史运行spark实例本地模式123/usr/local/spark/bin/run-example SparkPi &gt; SparkPiRes.txtvi SparkPiRes.txt在yarn集群上运行运行可以有client模式和cluster模式区别如下When run SparkSubmit —class [mainClass], SparkSubmit will call a childMainClass which isclient mode, childMainClass = mainClassstandalone cluster mde, childMainClass = org.apache.spark.deploy.Clientyarn cluster mode, childMainClass = org.apache.spark.deploy.yarn.ClientThe childMainClass is a wrapper of mainClass. The childMainClass will be called in SparkSubmit, and if cluster mode, the childMainClass will talk to the the cluster and launch a process on one woker to run the mainClass.ps. use “spark-submit -v” to print debug infos.Yarn client: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master yarn JavaWordCount.jarchildMainclass: org.apache.spark.examples.JavaWordCountYarn cluster: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master yarn-cluster JavaWordCount.jarchildMainclass: org.apache.spark.deploy.yarn.ClientStandalone client: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master spark://aa01:7077 JavaWordCount.jarchildMainclass: org.apache.spark.examples.JavaWordCountStanalone cluster: spark-submit -v —class “org.apache.spark.examples.JavaWordCount” —master spark://aa01:7077 —deploy-mode cluster JavaWordCount.jarchildMainclass: org.apache.spark.deploy.rest.RestSubmissionClient (if rest, else org.apache.spark.deploy.Client)Taking standalone spark as example, here is the client mode workflow. The mainclass run in the driver application which could be reside out of the cluster.client 模式结构On cluster mode showed as below, SparkSubmit will register driver in the cluster, and a driver process launched in one work running the main class.cluster模式结构yarn-client模式可以在运行中看见输出日志。如果是使用 yarn-cluster 模式，在运行过程中不会看到统计结果。需要到日志文件中查看。由于cluster模式，分配资源的driver不在本节点，无法把结果输出到命令行，但是通常cluster模式速度更快一些。因为driver是任务提交后自行分配的，资源和job分配更加合理。键入命令运行wordcount1/usr/local/spark/bin/spark-submit --deploy-mode cluster --master yarn --driver-memory 1G --class org.apache.spark.examples.JavaWordCount --executor-memory 1G --total-executor-cores 2 /usr/local/spark/lib/spark-examples-1.6.3-hadoop2.6.0.jar hdfs://master:9000/input出现下图表示程序无错误，运行成功本次测试使用的也是单词的统计输入如下1234567a.txtHello worldbye hadoopb.txtbye worldhello hadoop hadoop键入命令查看输出测试的输出结果/usr/local/hadoop/bin/yarn logs -applicationId XXX五、Scala与Sbt的配置和测试这篇博文解释得非常清楚~https://blog.csdn.net/a532672728/article/details/72477591六、Item-based Collaboration Filtering见另一篇博文七、SlopeOne见另一篇博文","categories":[{"name":"plantform","slug":"plantform","permalink":"http://zhongfuze.github.io/categories/plantform/"}],"tags":[{"name":"spark,hive,hadoop,scala","slug":"spark-hive-hadoop-scala","permalink":"http://zhongfuze.github.io/tags/spark-hive-hadoop-scala/"}]},{"title":"Paper-Reading-Attention-based Transactional Context Embedding for Next-Item Recommendation","slug":"Paper-Reading1","date":"2018-08-09T02:46:45.000Z","updated":"2018-08-15T07:31:00.469Z","comments":true,"path":"2018/08/09/Paper-Reading1/","link":"","permalink":"http://zhongfuze.github.io/2018/08/09/Paper-Reading1/","excerpt":"","text":"Paper Reading —— Attention-based Transactional Context Embedding for Next-Item Recommendation基于注意力的事务上下文嵌入下一项推荐``Abstract在电商交易环境中向user推荐下一个item,这样的应用非常实用但是具有挑战性。Transactional context 是指在交易记录中的observed items。大多数现有的推荐系统，主要是考虑recently occurring items 而不是 all the ones observed in the current context，这些算法通常假设交易中的items之间存在严格的顺序，但是这并不总是起作用，a long transaction(一个比较长的交易范围)通常包含许多对下一个选择的item没有关联或者说是没有用的item信息，这往往会overwhelm一些真正相关的item的影响。123456789101112131415举个栗子~让我们举一个例子来说明上述问题。用户首先将三个项目&#123;milk，apple，orange&#125;放入购物车中，然后将&#123;bread&#125;添加到同一购物车中。随后，交易被确定为&#123;milk，apple，orange，bread&#125;。如果我们将前三个项目作为上下文而最后一个项目作为推荐的目标，现有方法可能会建议&#123;vegetables&#125;，如&#123;green salad&#125;，因为最近的上下文项目（orange和apple）。但是，目标物品面包的选择可能取决于第一个项目（milk）。在这种情况下，推荐系统应该更多地关注milk而不是orange和apple，因为milk可能与下一个选择的bread更相关。此示例显示了下一项建议的重要性，这可能会被交易中的无关项误导。此外，真实世界的交易数据通常仅指示那些项目与项目之间的订单（例如，项目时间戳）共同出现在交易中。因此，推荐具有严格订单的交易项目可能是不可能和现实的。作者提出一个推荐算法，这个算法不仅考虑当前交易中所有的observed items，而且还要用不同的relevance(相关性)对它们进行加权，以建立一个attentive context(注意力上下文)，以高概率输出正确的下一个项目。模型——基于注意的事务嵌入模型（ATEM），用于上下文嵌入，以在不假定顺序的情况下对每个观察到的项目进行加权。对交易数据集的实证研究证明，ATEM在准确性和新颖性方面都显着优于最先进的方法。Main Algorithm问题描述与定义推荐基于购物车序列( built onshoppingbasket-basedtransactiondata)给定transactional dataset:T = \\{t_{1},t_{2},...,t_{|T|}\\}给定每个transactiont =\\{i_{1},i_{2},...,i_{|t|}\\}每个transaction是itemset的子集，且t里的并不是严格的交易顺序。给定(target item)目标 \\(i_{s}\\in t\\)，除了item \\(i_{s}\\)，所有属于\\(t\\)的items统称为context \\(c\\)，其中\\(c=i_{s}\\setminus t\\)。特别地，attentive context意味着上下文中的项目对下一项目推荐的上下文嵌入有不同的贡献。给定context \\(c\\)，本文的ATEM模型可以构建并训练Motivation推荐系统（RS）发挥着重要作用，特别是在商业领域。然而，大多数现有的RS理论面临各种问题，例如倾向于重复与用户可能已经选择的类似的项目。在现实数据场景中，用户可能更喜欢新颖且与手头上现有的不同的项目。为了解决这个问题，需要在交易上下文中进行新的推荐，即在交易中已经选择的内容中挖掘。一方面，分析基于RS交易序列，通过分析交易间耦合关系，产生更合理和可靠的新交易建议，例如下一个购物车(basket)和下一项目(item)建议。这些与基于user profile和item profile构建的典型RS方法完全不同。然而，当一个items集合被放入一个transaction时，仍然不清楚应该下一个项目应该推荐什么。这产生了通过分析事务内依赖性来推荐事务上下文下的下一项的需要。(transactional context:用于推荐下一个项目的上下文是指对应的项目相关交易，例如，由多个所选项目组成的购物篮记录)Related了解transaction context中items之间的相关性和转换非常具有挑战性。在TBRS中，一个普遍的挑战是建立一个注意力(attention)的背景，以高概率输出真正的下一个选择。一些现有方法旨在通过将transaction as the context来生成推荐。然而，大多数现有TBRS利用具有排序假设的部分上下文。顺序模式挖掘（2012）用于使用具有严格顺序假设的items之间的关联来预测下一项。但是，上下文中的项可能是任意的，这可能无法匹配任何已挖掘的模式。马尔可夫链（MC）（2012）是建模顺序数据的另一种方法。然而，MC只捕获从一个项目到下一个项目的转换，而不是从上下文序列中捕获，即，它只能捕获第一次转换。最近，基于矩阵隐式因子分解（MF）的方法（2016）将转移概率的矩阵从然而，由于现实世界中的幂律分布数据，MF很容易受到稀疏性问题的困扰（2016）。受Deep Learning的巨大成功的启发（2015），应用深度递归神经网络（RNN）来模拟顺序数据的事务，但由复杂结构引起的高计算成本阻止了其应用于大数据。此外，MC，MF和RNN最初是为具有严格自然顺序的时间序列数据而设计的，因此它们不具有无序的交易。(例如，或面包是否首先放入购物车中没有区别。另外，现有方法不能有效地加权上下文中的项目，即更多地关注那些相关项目。这种注意区分非常重要，特别是对于长期交易而言，这些交易往往包含许多与下一个选择无关的项目。)最近，受心理认知方案的启发，注意机制在上下文学习相关方面显示出惊人的潜力。 通过搜索图像中与答案相关的区域，呈现用于图像问题回答的堆叠注意网络（SAN）。 另一个新模型在人类关注的指导下学习句子表征（Shaonan，Jiajun和Chengqing 2017）。 鉴于CV和NLP中的上下文学习注意机制的巨大成功，我们结合了一些想法并提出ATEM来模拟下一个项目推荐的注意上下文。Contribution本文通过提出一种基于注意力的交易嵌入模型（ATEM）来解决需求。 ATEM通过识别与下一个选择具有高度相关性的上下文项，在交易中的所有观察项目的嵌入（Embedding）上构建了一个关注的上下文（attention context）。构建了一个浅宽的广泛网络（wide-in-wide-out network）（Goth 2016），以减少时间和空间成本。具体而言，作者将注意机制（Shaonan，Jiajun和Chengqing 2017）纳入浅层网络，以在没有严格排序假设的情况下在事务中构建所有观察项目（observed items）的注意上下文(attention context)。由于注意机制，所提出的模型能够更多地关注更相关的items，而更少关注不太相关的items。因此，ATEM更有效，更强大，可以预测具有较少约束的事务中的下一个item。这项工作的主要贡献如下：基于注意力的模型学习一种注意力的上下文嵌入，强化了相关项目但忽略了与下一个选择无关的项目。 我们的方法不涉及对事务中项目的严格排序假设。浅宽的宽广网络实现了ATEM，它对于大量项目的学习和预测更有效和高效。实证研究表明ATEM在准确性和新颖性方面明显优于两个真实数据集上的最新TBRS;通过比较有无注意机制的方法，注意机制对TBRS产生显着差异。Summary","categories":[{"name":"recommendation","slug":"recommendation","permalink":"http://zhongfuze.github.io/categories/recommendation/"}],"tags":[{"name":"embedding,recommendation","slug":"embedding-recommendation","permalink":"http://zhongfuze.github.io/tags/embedding-recommendation/"}]},{"title":"我的第一篇文章","slug":"我的第一篇文章","date":"2018-08-08T12:38:10.000Z","updated":"2018-08-10T07:41:02.290Z","comments":true,"path":"2018/08/08/我的第一篇文章/","link":"","permalink":"http://zhongfuze.github.io/2018/08/08/我的第一篇文章/","excerpt":"","text":"hexo 搭建过程node.js下载地址：http://nodejs.cn/download/git环境下载地址：https://gitforwindows.org/githubhttps://github.com创建文件夹我的文件夹建立在D盘打开Git Bash，输入：cd d:cd blog接着，输入npm install hexo如果出现说npm没有安装的话，请移步编辑用户环境变量，将npm加入环境变量中，重启git bash即可。再次进入文件夹执行安装命令，如果出现npm WARN deprecated titlecase@1.1.2: no longer maintained可以不用理会如果目录下出现文件夹就说明初始步骤基本成功了。hexo命令1234hexo init --初始化hexo环境,这时会在目录下自动生成hexo的文件npm install --安装npm依赖包hexo generate --生成静态页面hexo server --生成本地服务如果出现hexo command not found（笔者安装过程好坎坷啊！！！）解决方法：找到刚刚新建的blog文件夹，xxx/blog/hexo/bin将这个路径添加到环境变量(xxx为自定义的路径,笔者的就是D:/blog/hexo/bin)在blog路径下新建一个文件夹hexo执行hexo init需要在一个空的文件夹下执行，执行过程和结果如下图打开http://localhost:4000/看看是否启动成功，启动后会出现这个页面。发布local到internet使用github创建远程仓库，新建一个跟自己账号名字一样的空仓库接着打开本地的hexo文件夹中的_config.yml文件打开文件编辑文件末，将deploy修改如下：(使用自己repo地址)编辑完成后保存。下面的步骤如果不想新建文章的话，到这就结束了保存后再Git Bash 中执行``npm install hexo-deployer-git --save`` 最后执行 ``hexo deploy`` 这一步需要保证github上拥有本机的公钥！！！ 备注：添加公钥方法本地创建SSH Keysssh-keygen -t rsa -C &quot;xxx@xxx.com&quot;邮箱即为 github 注册邮箱,之后会要求确认路径和输入密码，一路回车。成功的话会在 ~/下生成 .ssh文件夹，进去，打开 id_rsa.pub，复制里面的key即可。cat ~/.ssh/id_rsa.pub复制内容至github-Setting-GPG and SSH Keys-Add新建文章在Git Bash输入hexo new World注：World 是标题在xxx/blog/hexo/source/_post文件夹里会有World.md内容如下：在之后使用md格式编辑正文并保存。hexo 部署12//生成静态文件hexo generate12//安装 hexo-deployer-gitnpm install hexo-deployer-git --save12//部署到githubhexo deploy打开http://zhongfuze.github.io完毕Themehttps://hexo.io/themes/mathjaxhttps://blog.csdn.net/u014630987/article/details/78670258markdownmarkdownpad ： http://markdownpad.com/download.htmlawesomium ： awesomium_v1.6.6_sdk_win.exe","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2018-08-08T09:11:56.371Z","updated":"2018-08-08T09:11:56.373Z","comments":true,"path":"2018/08/08/hello-world/","link":"","permalink":"http://zhongfuze.github.io/2018/08/08/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new \"My New Post\"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment","categories":[],"tags":[]}]}